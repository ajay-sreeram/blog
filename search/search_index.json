{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Code &amp; Conversations","text":"<p>Welcome to my personal blog where I share my thoughts, research, code snippets, and more!</p> <pre><code>    I am still thinking of how to present this page well\n    For now, Please click on above tabs and find some interesting things\n</code></pre>"},{"location":"about/","title":"About Me","text":"<p>Hello and welcome to Code &amp; Conversations! My name is Ajay, and I am passionate about the intersection of technology, language, and innovation.</p> <p>With expertise in Natural Language Processing and a flair for Full Stack Development, I've ventured into creating chatbots, conversational agents, websites and even mobile applications. My journey has also seen me dabbling in software development projects ranging from Python web crawlers to Neural website creation.</p> <p>But that's not all! My weekends often find me immersed in personal projects, like designing a voice bot with multifaceted capabilities or crafting an augmented reality app that brings mythical figures to life. </p> <p>I've been fortunate to win hackathon challenges, develop voice bots, and even conceptualize dance steps through the magic of code. Every project, be it a hackathon challenge or a weekend experiment, tells a story of innovation and exploration.</p> <p>Join me as I share my adventures in the world of Code &amp; Conversations. Let's explore, learn, and innovate together!</p> <p>Connect with me on LinkedIn | Follow me on Twitter | Check out my GitHub</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/one/","title":"this is one","text":""},{"location":"blog/three%20four/","title":"Three four","text":"<p>This is Three+Four</p>"},{"location":"blog/two/","title":"This is two","text":""},{"location":"coding/","title":"Are you a Programmer?","text":"<p>Here are some learnings to enhance your coding abilities</p> <p></p> <p>CodeLLama: Potential of code generation models</p> <p>How Code generation models can enhance developer productivity?</p> <p> </p> <p></p> <p>Decorators: Decorators are like developers crown</p> <p>How to use decorators effectively?</p>"},{"location":"coding/decorators/","title":"Decorators: Decorators are like developers crown","text":""},{"location":"coding/decorators/#introduction-to-decorators","title":"Introduction to Decorators","text":"<p>Have you ever wanted to add extra behavior to a function without actually modifying its code? That's what decorators in Python allow you to do. Decorators act like wrappers that modify the behavior of the function they wrap. To put it simply, consider this example:</p> <pre><code>def simple_decorator(func):\n    def wrapper():\n        print(\"Something is happening before the function is called.\")\n        func()\n        print(\"Something is happening after the function is called.\")\n    return wrapper\n\n@simple_decorator\ndef say_hello():\n    print(\"Hello!\")\n\nif __name__ == '__main__':\n    say_hello()\n\n# Output:\n# Something is happening before the function is called.\n# Hello!\n# Something is happening after the function is called.\n</code></pre> <p>The @simple_decorator is altering the behavior of say_hello() by adding print statements before and after its execution.</p>"},{"location":"coding/decorators/#what-can-decorators-do-for-you","title":"What Can Decorators Do For You?","text":"<p>Decorators serve as versatile tools, capable of logging, enforcing access control, caching, and more. They enable you to segregate responsibilities in your codebase. Essentially, they can add features to your functions or classes without altering their structure, making your code more modular and easier to manage.</p>"},{"location":"coding/decorators/#logging-function-calls","title":"Logging Function Calls","text":"<p>Imagine you're deep into debugging, and you want to understand which functions are being called, along with the arguments they receive. Here's where a logging decorator can come in handy:</p> <pre><code>import functools\n\ndef log_function_calls(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        print(f\"Calling {func.__name__} with arguments: {args}, {kwargs}\")\n        result = func(*args, **kwargs)\n        print(f\"{func.__name__} returned: {result}\")\n        return result\n    return wrapper\n\n@log_function_calls\ndef add(a, b):\n    return a + b\n</code></pre> <p>This logs function names, arguments, and return values, saving you from littering your codebase with numerous print statements.</p>"},{"location":"coding/decorators/#debugging-and-timing","title":"Debugging and Timing","text":"<p>You might sometimes wonder why a particular piece of code is taking too long to execute. Rather than manually calculating the execution time, why not use a decorator to do that?</p> <pre><code>import functools\nimport time\n\ndef debug_with_timing():\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            start_time = time.time()\n            result = func(*args, **kwargs)\n            end_time = time.time()\n            elapsed_time = end_time - start_time\n            print(f\"Function {func.__name__} took {elapsed_time:.6f} seconds to execute.\")\n            return result\n        return wrapper\n    return decorator\n\n@debug_with_timing\ndef long_running_function():\n    time.sleep(2)\n    return \"Done\"\n</code></pre> <p>This decorator measures the time taken by a function to execute and logs it, helping you identify bottlenecks.</p>"},{"location":"coding/decorators/#marking-functions-as-deprecated","title":"Marking Functions as Deprecated","text":"<p>So, you've found a more efficient way of doing something and no longer want anyone to use the old method. How do you warn them? The answer is a decorator that marks the old function as deprecated.</p> <pre><code>import functools\nimport warnings\n\ndef deprecated(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        warnings.warn(f\"Function {func.__name__} is deprecated.\", \n                      category=DeprecationWarning, \n                      stacklevel=2)\n        return func(*args, **kwargs)\n    return wrapper\n\n@deprecated\ndef old_function():\n    return \"This function is outdated.\"\n</code></pre> <p>Using this decorator, a deprecation warning is issued whenever the function is called.</p>"},{"location":"coding/decorators/#implementing-retries","title":"Implementing Retries","text":"<p>We all know that certain operations, especially the ones involving networks or databases, can fail sporadically. How about a decorator that allows your function to retry a few times before giving up?</p> <pre><code>import functools\nimport time\n\ndef retry_on_failure(max_retries=3, delay=1):\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            for _ in range(max_retries):\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    print(f\"Failed with error: {e}. Retrying in {delay} seconds...\")\n                    time.sleep(delay)\n            raise Exception(f\"Function {func.__name__} failed after {max_retries} retries.\")\n        return wrapper\n    return decorator\n\n@retry_on_failure()\ndef api_request(url):\n    # Code for making an API request goes here\n    ...\n</code></pre> <p>This decorator will attempt to re-run the function up to a specified number of times if it encounters an error, making your system more resilient to transient failures.</p>"},{"location":"coding/decorators/#memoization","title":"Memoization","text":"<p>Have a function that gets called multiple times with the same arguments? You might be wasting computation cycles recalculating the same values again and again. Here's where memoization comes in handy:</p> <pre><code>import functools\n\ndef memoize(func):\n    cache = {}\n    @functools.wraps(func)\n    def wrapper(*args):\n        if args not in cache:\n            cache[args] = func(*args)\n        return cache[args]\n    return wrapper\n\n@memoize\ndef fibonacci(n):\n    if n &lt;= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n</code></pre> <p>This decorator caches the return value of functions, so if you call it again with the same arguments, the cached value is returned instead of re-running the function.</p>"},{"location":"coding/decorators/#enforcing-access-control","title":"Enforcing Access Control","text":"<p>Security is a big deal. We often need to restrict who can do what in our applications. This decorator helps enforce such access controls:</p> <pre><code>import functools\n\ndef is_authenticated():\n    # Authentication check\n    ...\n\ndef requires_authentication(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        if is_authenticated():\n            return func(*args, **kwargs)\n        else:\n            raise PermissionError(\"Authentication required.\")\n    return wrapper\n\n@requires_authentication\ndef sensitive_operation():\n    # Code for sensitive operation goes here\n    ...\n</code></pre> <p>This decorator checks for authentication and only proceeds with function execution if the user is authenticated, safeguarding sensitive operations in your system.</p>"},{"location":"coding/decorators/#conclusion","title":"Conclusion","text":"<p>Decorators offer a powerful, flexible way to modify the behavior of your functions or methods. They can be your best friend for debugging, performance tuning, access control, and many other tasks. By using them effectively, you're well on your way to writing cleaner, more efficient, and more maintainable Python code.</p> <p>Hope you found this enlightening! Happy Coding!</p>"},{"location":"coding/potential_of_code_generation_language_models/","title":"Empowering Developers: Unleashing the Potential of Code Generation Language Models","text":"<p>We are all familiar with the impressive capabilities of ChatGPT in tasks like generating code from English descriptions or completing partial code snippets. However, a significant limitation is the dependency on online OpenAI APIs, preventing offline usage. Despite OpenAI's assurance that user data isn't used for training subsequent versions, concerns over security have restrained major companies from fully embracing these models for coding tasks.</p> <p>Enter Codellama - a dedicated AI model fine-tuned specifically for code generation. Not only is it free to use, but it's also a lightweight solution that can operate offline on local machines. This breakthrough empowers corporations to harness the prowess of language models while keeping operations secure within their networks.</p> <p>What's more, the rapid evolution in this field is undeniable. Almost immediately after Codellama's release, a finely tuned variant named WizardCoder emerged, outperforming GPT-4 on select code datasets. These advancements underline the urgency of tapping into the full spectrum of these models' capabilities to enhance overall productivity in software development.</p>"},{"location":"coding/potential_of_code_generation_language_models/#in-practice","title":"In Practice","text":"<p>We all acknowledge that language models are renowned for their effectiveness in code completion. Yet, it's crucial to acknowledge that their potential reaches much further. These models present a diverse range of developer-centric applications that amplify productivity and streamline efficiency throughout every phase of the coding journey.</p> <p>Here are some effective ways we can leverage these models to simplify our coding tasks and achieve even greater productivity:</p>"},{"location":"coding/potential_of_code_generation_language_models/#automated-testing","title":"Automated Testing","text":"<p>Writing unit tests and integration tests are crucial for maintaining code quality. Code generation models can assist in generating test cases and assertions based on code functionality.</p>"},{"location":"coding/potential_of_code_generation_language_models/#code-translation","title":"Code Translation","text":"<p>For international teams, or when working with codebases in different languages, code translation can be a challenge. Code generation models can help translate code snippets from one programming language to another, saving time and effort.</p>"},{"location":"coding/potential_of_code_generation_language_models/#refactoring-assistance","title":"Refactoring Assistance","text":"<p>Refactoring code to improve readability, performance, or maintainability can be a complex task. Code generation models can suggest refactoring options, provide alternative implementations, and highlight potential areas for improvement.</p>"},{"location":"coding/potential_of_code_generation_language_models/#code-documentation","title":"Code Documentation","text":"<p>Writing comprehensive and clear code comments and documentation is essential for collaborative development. Code generation models can help generate explanatory comments in different languages for functions, classes, and modules, improving the overall quality of the codebase.</p>"},{"location":"coding/potential_of_code_generation_language_models/#uml-diagram-generation","title":"UML Diagram Generation","text":"<p>Code generation models can be employed to automatically generate Unified Modeling Language (UML) diagrams from code or textual descriptions. This can include class diagrams, sequence diagrams, activity diagrams, and more, aiding in visualizing the structure and behavior of software systems. This is particularly useful for documenting complex architectures and facilitating communication between developers and stakeholders.</p>"},{"location":"coding/potential_of_code_generation_language_models/#bug-fixes","title":"Bug Fixes","text":"<p>Developers can use code generation models to identify and fix bugs in their code. By describing the issue, the model can provide insights into potential solutions, helping to troubleshoot problems more efficiently.</p>"},{"location":"coding/potential_of_code_generation_language_models/#code-generation","title":"Code Generation","text":"<p>Developers might sometimes have high-level ideas but struggle to translate them into code. Code generation models can take natural language descriptions of a task and generate corresponding code snippets.</p>"},{"location":"coding/potential_of_code_generation_language_models/#code-complexity-reduction","title":"Code Complexity Reduction","text":"<p>Code generation models can provide suggestions for simplifying complex code blocks, and improving code readability and maintainability.</p>"},{"location":"coding/potential_of_code_generation_language_models/#mock-data-generation","title":"Mock Data Generation","text":"<p>During development and testing, creating mock data can be time-consuming. Code generation models can help generate realistic mock data for testing purposes.</p>"},{"location":"coding/potential_of_code_generation_language_models/#version-migration","title":"Version Migration","text":"<p>When migrating to a new version of a programming language or framework, code generation models can help identify and update code that needs modification due to changes in syntax or API.</p>"},{"location":"coding/potential_of_code_generation_language_models/#api-usage-examples","title":"API Usage Examples","text":"<p>When working with new libraries or APIs, developers might struggle with understanding the correct usage. Code generation models can provide real-world examples of how to interact with different APIs, making it easier to integrate them into projects.</p>"},{"location":"coding/potential_of_code_generation_language_models/#data-parsing-and-serialization","title":"Data Parsing and Serialization","text":"<p>Code generation models can assist in generating code to parse and serialize data in various formats such as JSON, XML, CSV, and more.</p>"},{"location":"coding/potential_of_code_generation_language_models/#conclusion","title":"Conclusion","text":"<p>These use cases underscore the remarkable versatility of code generation language models and their potential to seamlessly integrate into diverse stages of the software development lifecycle. By leveraging these capabilities, developers can significantly enhance their efficiency, code quality, and collaborative efforts.</p> <p>However, it's important to acknowledge that code generation models cannot be solely relied upon without consideration. Instances of generating buggy code can still arise. Consequently, IDEs must undergo updates to seamlessly integrate the extensive capabilities of language models, as highlighted in the mentioned use cases. Equally vital is to ensure that developers maintain convenient access to validate the content of the generated code. Striking this balance between automation and human oversight will be pivotal in fully capitalizing on these models' potential and driving the software development landscape towards enhanced efficiency and reliability.</p> <p>Happy coding! \u263a</p>"},{"location":"learning/","title":"Learning","text":"<p>Thinking...</p> <p>Interesting things are on their way</p>"},{"location":"research/","title":"Are you a Data Scientist?","text":"<p>Here are some learnings to enhance your technical understandings</p> <p></p> <p>Transformer: A Simple understanding of Transformer</p> <p>Do you want to understand the components of transformer without any jargons?</p> <p> </p> <p></p> <p>Navigating the Landscape of Modern NLP</p> <p>Do you want to get an overview of recent progress in making LLM's better?</p> <p>... and so on</p>"},{"location":"research/landscape_of_modern_nlp/","title":"Navigating the Landscape of Modern NLP","text":"<p>The landscape of Natural Language Processing (NLP) has been deeply transformed by the rise of Transformer models. Their unparalleled effectiveness on a wide range of tasks has led to rapid advancements in the field. However, with great power comes great responsibility, or in this case, complexity and limitations. As an experienced NLP scientist, let's delve deep into the nuances of Transformers and their recent advancements, explaining how they influence not just text generation platforms like ChatGPT but also industry-wide applications.</p>"},{"location":"research/landscape_of_modern_nlp/#vanilla-transformers","title":"Vanilla Transformers","text":"<p>The Transformer architecture has been the cornerstone of modern Natural Language Processing (NLP). It comprises several building blocks that make it uniquely suited for a variety of tasks and lets focus on few:</p> <ul> <li> <p>Positional Embeddings: These provide a way to add the sequence order information since the transformer itself doesn't understand the order of words.</p> </li> <li> <p>Attention Mechanism: This is the magic sauce. It enables the model to focus on different parts of the input text when producing an output, which is crucial for understanding the context.</p> </li> <li> <p>Activation and Normalization: Activation functions introduce non-linearity into the system, and normalization helps in faster and more stable training.</p> </li> </ul> <p>Each of these blocks plays a pivotal role in making Transformers what they are: incredibly effective but computationally demanding.</p>"},{"location":"research/landscape_of_modern_nlp/#the-long-sequence-challenges","title":"The Long Sequence Challenges","text":"<p>However, these building blocks are not without their drawbacks. The positional embeddings have limitations when it comes to very long sequences because of their fixed-size nature, and the attention mechanism is computationally intensive, having quadratic complexity. These challenges have been quite restrictive, especially when working with long texts or requiring fast real-time responses.</p>"},{"location":"research/landscape_of_modern_nlp/#tackling-attention-on-long-sequences","title":"Tackling Attention on Long Sequences:","text":"<p>These methods primarily focus on enabling the transformer architecture to manage long sequences without overwhelming computational and memory resources.</p> <ul> <li> <p>Rotary positional embeddings (RoPE): The key idea behind RoPE is to use a rotation matrix to encode absolute positions while also incorporating explicit relative position dependencies in the self-attention mechanism.</p> </li> <li> <p>Alibi positional embeddings: ALiBi proposes a much simpler relative position encoding scheme. The relative distance that input tokens have to each other is added as a negative integer scaled by a pre-defined value m to each query-key entry.</p> </li> <li> <p>Dilated Attention: This method spaces out the attention heads, allowing the model to capture long-range dependencies without having to attend to every single token. This reduces computational overhead and allows the model to work with longer sequences.</p> </li> <li> <p>Sliding Window Attention: In this technique, each token in the sequence pays attention only to a subset of nearby tokens rather than all tokens. This localized approach makes it feasible to work with longer sequences.</p> </li> <li> <p>Attention Sinks: Preserve a few initial tokens' KV alongside the sliding window's KV in the attention computation. These initial tokens serve as attention sinks (tokens that receive these unnecessary attention values), stabilizing the attention distribution and maintaining the model's performance even when the cache size is exceeded.</p> </li> </ul> <p>While advancements have addressed sequence length, Large Language Models (LLMs) still pose significant challenges during inference.</p>"},{"location":"research/landscape_of_modern_nlp/#techniques-helping-in-memory-consumption","title":"Techniques Helping in Memory Consumption","text":"<p>These methods are designed to optimize the computational efficiency of transformers, significantly reducing the memory footprint while maintaining model performance.</p> <ul> <li> <p>Flash Attention: This approach breaks down the computation into smaller chunks. It provides outputs identical to the default self-attention layer but with memory costs that increase only linearly.</p> </li> <li> <p>Key-Value Caching: This technique involves storing the key-value pairs from previous attention computations. When attending to similar sequences, the model can simply reuse these stored pairs, significantly reducing the computational load. Multi-Query and Grouped Query Attention: In Multi-Query Attention, all attention heads share a single set of key-value projection weights, optimizing memory consumption. Grouped Query Attention mitigates the quality drop associated with this by employing a limited set of distinct key-value weights for different groups of heads, thus achieving a compromise between performance and efficiency.</p> </li> <li> <p>Assisted Generation: The concept involves using a smaller 'assistant' model to rapidly generate candidate tokens. The main model then confirms these tokens.</p> </li> <li> <p>Quantization: Reduce the number of bits required to represent weights and activations of neural networks. Operating at reduced numerical precision, namely 8-bit and 4-bit, can achieve computational advantages without a considerable decline in model performance.</p> </li> </ul>"},{"location":"research/landscape_of_modern_nlp/#lightweight-models-and-fine-tuning","title":"Lightweight Models and Fine-Tuning","text":"<p>The solution lies in creating lightweight models and fine-tuning them for specific tasks. Fine-tuning helps to adapt a large pre-trained model to more specific tasks efficiently. Techniques like LoRA, Adapters, and Prefix Tuning have shown promise in this area.</p> <ul> <li>LoRA: Fine-tunes attention patterns for specific tasks.</li> <li>Adapters: Small neural modules added for task-specific fine-tuning.</li> <li>Prefix Tuning: Adds a trainable prefix to adapt the pre-trained model for specific tasks.</li> </ul>"},{"location":"research/landscape_of_modern_nlp/#unlocking-full-capabilities-prompt-engineering","title":"Unlocking Full Capabilities: Prompt Engineering","text":"<p>Now that we have well-performing, fine-tuned, and compressed models, how do we extract their full capabilities? The answer lies in Prompt Engineering.</p> <p>Prompt engineering is the practice of designing inputs for generative AI tools that will produce optimal outputs. Some of the recent advances in this area are:</p> <ul> <li> <p>Tree of thoughts: It guides language models in solving complex problems by structuring the reasoning process into a tree of intermediate thoughts or steps. These thoughts are systematically explored, evaluated, and ranked to find the most promising solutions.</p> </li> <li> <p>Prompt Breeder: It works by creating a sample population of prompts and iteratively improving the prompts by performing mutation and allowing the best prompts to survive.</p> </li> <li> <p>AutoGen: We can have multiple AI agents working together on a common task.</p> </li> </ul>"},{"location":"research/landscape_of_modern_nlp/#future-landscape","title":"Future Landscape","text":"<p>As we move forward, these advancements will continue to democratize access to LLMs, making them an integral part of our daily lives from personal assistants to advanced analytics tools. The horizon looks promising, and we are only scratching the surface.</p> <p>Thank you, and stay tuned for more!</p>"},{"location":"research/transformer_review/","title":"A Simple understanding of Transformer","text":"<p>The \ud835\udde7\ud835\uddff\ud835\uddee\ud835\uddfb\ud835\ude00\ud835\uddf3\ud835\uddfc\ud835\uddff\ud835\uddfa\ud835\uddf2\ud835\uddff architecture is the fundamental building block that enables LLM's to understand and generate text. Let's break it down as if we're reading a magical storybook. Each component of a Transformer model is like a unique character in this captivating tale!</p> <p>\ud835\udddc\ud835\uddfb\ud835\uddfd\ud835\ude02\ud835\ude01 \ud835\uddee\ud835\uddfb\ud835\uddf1 \ud835\udde2\ud835\ude02\ud835\ude01\ud835\uddfd\ud835\ude02\ud835\ude01: Imagine you have a magic storytelling book. You tell it a sentence (input), and it gives you a new sentence (output) that continues the story.</p> <p>\ud835\uddd8\ud835\uddfa\ud835\uddef\ud835\uddf2\ud835\uddf1\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\udddf\ud835\uddee\ud835\ude06\ud835\uddf2\ud835\uddff: First, each word in the sentence you tell the magic book gets turned into a special sticker. These stickers help the book understand what each word really means.</p> <p>\ud835\udde3\ud835\uddfc\ud835\ude00\ud835\uddf6\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb\ud835\uddee\ud835\uddf9 \ud835\uddd8\ud835\uddfb\ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4: To make sure the book knows the order of the words in your sentence, each sticker gets a number tag. So the book knows \"the cat ate\" is different from \"ate the cat.\"</p> <p>\ud835\uddd4\ud835\ude01\ud835\ude01\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\udde0\ud835\uddf2\ud835\uddf0\ud835\uddf5\ud835\uddee\ud835\uddfb\ud835\uddf6\ud835\ude00\ud835\uddfa: The book has a magical magnifying glass. It uses it to look at all the stickers (words) you gave it, but it pays more 'attention' to the most important ones for understanding the story.</p> <p>\ud835\udde0\ud835\ude02\ud835\uddf9\ud835\ude01\ud835\uddf6-\ud835\udddb\ud835\uddf2\ud835\uddee\ud835\uddf1 \ud835\uddd4\ud835\ude01\ud835\ude01\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb: Imagine not just one, but several magical magnifying glasses looking at different parts of your sentence at the same time. This helps the book get a really good understanding of what you said.</p> <p>\ud835\uddd9\ud835\uddf2\ud835\uddf2\ud835\uddf1-\ud835\uddd9\ud835\uddfc\ud835\uddff\ud835\ude04\ud835\uddee\ud835\uddff\ud835\uddf1 \ud835\udddf\ud835\uddee\ud835\ude06\ud835\uddf2\ud835\uddff: It's like the book's \"editor,\" taking all the important words from your sentence and crafting them into a polished new line for the story.</p> <p>\ud835\udde1\ud835\uddfc\ud835\uddff\ud835\uddfa\ud835\uddee\ud835\uddf9\ud835\uddf6\ud835\ude07\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb: Sometimes the book gets too excited and starts talking really loudly or softly. Normalization is like a volume knob that makes sure everything sounds just right.</p> <p>\ud835\uddd7\ud835\uddf2\ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddff: Now, the book uses another magical tool to turn its thoughts into a new sentence that it can tell you, continuing the story.</p>"},{"location":"tech-updates/","title":"Tech Updates","text":"<p>Watch my likes in Twitter</p>"}]}