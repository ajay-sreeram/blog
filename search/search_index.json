{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Code &amp; Conversations","text":"<p>Welcome to my personal blog where I share my thoughts, research, code snippets, and more!</p> <p></p>"},{"location":"about/","title":"About Me","text":"<p>Hello and welcome to Code &amp; Conversations! My name is Ajay, and I am passionate about the intersection of technology, language, and innovation.</p> <p>With expertise in Natural Language Processing and a flair for Full Stack Development, I've ventured into creating chatbots, conversational agents, websites and even mobile applications. My journey has also seen me dabbling in software development projects ranging from Python web crawlers to Neural website creation.</p> <p>But that's not all! My weekends often find me immersed in personal projects, like designing a voice bot with multifaceted capabilities or crafting an augmented reality app that brings mythical figures to life. </p> <p>I've been fortunate to win hackathon challenges, develop voice bots, and even conceptualize dance steps through the magic of code. Every project, be it a hackathon challenge or a weekend experiment, tells a story of innovation and exploration.</p> <p>Join me as I share my adventures in the world of Code &amp; Conversations. Let's explore, learn, and innovate together!</p> <p>Connect with me on LinkedIn | Follow me on Twitter | Check out my GitHub</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/one/","title":"this is one","text":""},{"location":"blog/three%20four/","title":"Three four","text":"<p>This is Three+Four</p>"},{"location":"blog/two/","title":"This is two","text":""},{"location":"codes/","title":"Code Snippets","text":"<p>Here are some code snippets to understand ML basics</p> <p>Linear Regression</p> <p>Logistic Regression</p> <p>Kth Nearest Neighbour</p> <p>Linear Support Vector Machine</p> <p>Ada Boost</p> <p>Gradient Boost</p>"},{"location":"codes/ada_boost/","title":"Ada boost","text":"<pre><code># AdaBoost for binary classification\n# Assume data X with labels y (labels are -1 or 1)\n# Assume a function weak_learner() that trains a weak model on weighted data\n\ndef decision_stump_ada(X, y, weights):\n    best_feature, best_threshold = None, None\n    best_error = float('inf')\n\n    N, M = X.shape  # N samples, M features\n\n    for feature in range(M):\n        thresholds = np.unique(X[:, feature])\n        for threshold in thresholds:\n            # Example prediction: if less than threshold, predict -1, else 1\n            predictions = np.ones(N)\n            predictions[X[:, feature] &lt; threshold] = -1\n\n            # Weighted error\n            error = np.sum(weights[predictions != y])\n\n            if error &lt; best_error:\n                best_error = error\n                best_feature = feature\n                best_threshold = threshold\n\n    # Return a simple model based on the best feature and threshold\n    def stump_predict(X):\n        predictions = np.ones(X.shape[0])\n        predictions[X[:, best_feature] &lt; best_threshold] = -1\n        return predictions\n\n    return stump_predict\n\n\ndef AdaBoost(X, y, T):\n    N = len(y)  # Number of samples\n    w = np.ones(N) / N  # Initial weights\n\n    models = []  # To store weak learners\n    model_weights = []  # To store model weights\n\n    for t in range(T):  # Iterate T times\n        model = decision_stump_ada(X, y, w)  # Train weak_learner using weighted samples\n        predictions = model.predict(X)\n\n        # Calculate error and model weight\n        err = np.sum(w * (predictions != y)) / np.sum(w)\n        alpha = 0.5 * np.log((1 - err) / err)\n\n        # Update weights\n        w = w * np.exp(-alpha * y * predictions)\n        w /= np.sum(w)  # Normalize weights\n\n        models.append(model)\n        model_weights.append(alpha)\n\n    # Final model: return a function that aggregates weak learners\n    def final_model(X):\n        final_predictions = np.zeros(len(X))\n        for model, alpha in zip(models, model_weights):\n            final_predictions += alpha * model.predict(X)\n        return np.sign(final_predictions)\n\n    return final_model\n</code></pre>"},{"location":"codes/gradient_boost/","title":"Gradient boost","text":"<pre><code># Gradient Boost for regression\n# Assume data X with continuous labels y\n# Assume a function weak_learner() that trains a weak model on the data\ndef simple_decision_stump_grad(X, residuals):\n    best_feature = 0  # Arbitrarily pick the first feature for simplicity\n    best_threshold = np.median(X[:, best_feature])  # Use median as a simple threshold\n    left_mean = np.mean(residuals[X[:, best_feature] &lt; best_threshold])\n    right_mean = np.mean(residuals[X[:, best_feature] &gt;= best_threshold])\n\n    # Predictor based on the simple split\n    def stump_predict(X):\n        predictions = np.where(X[:, best_feature] &lt; best_threshold, left_mean, right_mean)\n        return predictions\n\n    return stump_predict\n\n\ndef GradientBoost(X, y, T):\n    # Initial prediction (mean of labels)\n    F0 = np.mean(y)\n    models = [F0]\n\n    for t in range(T):  # Iterate T times\n        # Calculate residuals\n        residuals = y - np.sum([m.predict(X) if hasattr(m, 'predict') else m for m in models], axis=0)\n\n        # Train a new model on residuals\n        model = simple_decision_stump_grad(X, residuals)\n        models.append(model)\n\n    # Final model: return a function that aggregates predictions\n    def final_model(X):\n        return np.sum([m.predict(X) if hasattr(m, 'predict') else m for m in models], axis=0)\n\n    return final_model\n</code></pre>"},{"location":"codes/kmeans_clustering/","title":"Kmeans clustering","text":"<pre><code>import numpy as np\n\nclass SimpleKMeans:\n    def __init__(self, K=2, max_iters=100, random_state=42):\n        self.K = K\n        self.max_iters = max_iters\n        self.random_state = random_state\n        self.centroids = []\n\n    def initialize_centroids(self, X):\n        np.random.seed(self.random_state)\n        random_idxs = np.random.permutation(X.shape[0])\n        centroids = X[random_idxs[:self.K]]\n        return centroids\n\n    def closest_centroid(self, X, centroids):\n        distances = np.sqrt(((X - centroids[:, np.newaxis])**2).sum(axis=2))\n        return np.argmin(distances, axis=0)\n\n    def compute_centroids(self, X, labels):\n        centroids = np.zeros((self.K, X.shape[1]))\n        for k in range(self.K):\n            centroids[k, :] = np.mean(X[labels == k, :], axis=0)\n        return centroids\n\n    def fit(self, X):\n        self.centroids = self.initialize_centroids(X)\n        for _ in range(self.max_iters):\n            labels = self.closest_centroid(X, self.centroids)\n            self.centroids = self.compute_centroids(X, labels)\n        self.labels = labels\n\n    def predict(self, X):\n        return self.closest_centroid(X, self.centroids)\n\n# Example usage\nif __name__ == \"__main__\":\n    from sklearn.datasets import make_blobs\n    import matplotlib.pyplot as plt\n\n    X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n\n    kmeans = SimpleKMeans(K=4)\n    kmeans.fit(X)\n</code></pre>"},{"location":"codes/knn/","title":"Knn","text":"<pre><code>import numpy as np\nfrom collections import Counter\n\ndef euclidean_distance(x1, x2):\n    \"\"\"\n    Compute the Euclidean distance between two points.\n    \"\"\"\n    return np.sqrt(np.sum((x1 - x2) ** 2))\n\ndef knn(X_train, y_train, X_test, k=3):\n    \"\"\"\n    k-Nearest Neighbor algorithm.\n\n    Args:\n    - X_train: Training data features\n    - y_train: Training data labels\n    - X_test: Test data (single data point or a collection of data points)\n    - k: Number of nearest neighbors to consider\n\n    Returns:\n    - predictions: The predicted labels for the test data\n    \"\"\"\n    predictions = []\n    for x_test in X_test:\n        # Calculate distances between x_test and all points in X_train\n        distances = [euclidean_distance(x_test, x_train) for x_train in X_train]\n\n        # Get indices of k smallest distances\n        k_indices = np.argsort(distances)[:k]\n\n        # Get the labels of the k nearest neighbors\n        k_nearest_labels = [y_train[i] for i in k_indices]\n\n        # Majority vote, most common class label among k nearest neighbors\n        most_common = Counter(k_nearest_labels).most_common(1)\n        predictions.append(most_common[0][0])\n\n    return predictions\n</code></pre>"},{"location":"codes/linear_regression/","title":"Linear regression","text":"<pre><code>import numpy as np\n\ndef linear_regression_from_scratch(x, y):\n    \"\"\"\n    Simple linear regression using least squares.\n\n    Args:\n    - x: numpy array or a list of independent variable.\n    - y: numpy array or a list of dependent variable.\n\n    Returns:\n    - coefficients: Tuple containing slope and intercept for the best fit line.\n    \"\"\"\n    # Convert lists to numpy arrays if necessary\n    x = np.array(x)\n    y = np.array(y)\n\n    # Calculating the mean of x and y\n    mean_x = np.mean(x)\n    mean_y = np.mean(y)\n\n    # Total number of values\n    n = len(x)\n\n    # Using the formula to calculate 'm' and 'c'\n    numerator = 0\n    denominator = 0\n    for i in range(n):\n        numerator += (x[i] - mean_x) * (y[i] - mean_y)\n        denominator += (x[i] - mean_x) ** 2\n\n    m = numerator / denominator  # Slope of the line\n    c = mean_y - (m * mean_x)  # Intercept of the line\n\n    return (m, c)\n\n# Example usage\nx = [1, 2, 3, 4, 5]\ny = [2, 3, 5, 7, 11]\n\nslope, intercept = linear_regression_from_scratch(x, y)\nprint(f\"Slope: {slope}, Intercept: {intercept}\")\n</code></pre>"},{"location":"codes/linear_svm/","title":"Linear svm","text":"<pre><code>import numpy as np\n\nclass SimpleSVM:\n    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n        self.lr = learning_rate\n        self.lambda_param = lambda_param\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        y_ = np.where(y &lt;= 0, -1, 1)\n\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n        \"\"\"\n        During training, we iterate through each sample in the dataset. \n        The condition checks if the sample is on the correct side of the margin. \n        If it is, we only update the weights based on the regularization term. \n        If it's not, we update the weights and bias based on both the regularization term and the misclassification.\n        \"\"\"\n        for _ in range(self.n_iters):\n            for idx, x_i in enumerate(X):\n                condition = y_[idx] * (np.dot(x_i, self.weights) - self.bias) &gt;= 1\n                if condition:\n                    self.weights -= self.lr * (2 * self.lambda_param * self.weights)\n                else:\n                    self.weights -= self.lr * (2 * self.lambda_param * self.weights - np.dot(x_i, y_[idx]))\n                    self.bias -= self.lr * y_[idx]\n\n    def predict(self, X):\n        linear_output = np.dot(X, self.weights) - self.bias\n        return np.sign(linear_output)\n</code></pre>"},{"location":"codes/logistic_regression/","title":"Logistic regression","text":"<pre><code>import numpy as np\n\ndef sigmoid(z):\n    \"\"\"\n    Apply sigmoid function.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\n\ndef cost_function(X, y, weights):\n    \"\"\"\n    Compute the cost function for all the training samples.\n    \"\"\"\n    m = X.shape[0]\n    h = sigmoid(X @ weights)\n    epsilon = 1e-5  # to avoid log(0)\n    cost = (-1/m) * ((y.T @ np.log(h + epsilon)) + ((1 - y).T @ np.log(1 - h + epsilon)))\n    return cost\n\ndef gradient_descent(X, y, weights, alpha, iterations):\n    \"\"\"\n    Perform gradient descent to learn weight parameters.\n    \"\"\"\n    m = X.shape[0]\n    cost_history = []\n\n    for i in range(iterations):\n        # Update weights based on output from sigmoid\n        weights = weights - (alpha/m) * X.T @ (sigmoid(X @ weights) - y)\n        # Calculate cost so we can see the change, here i am using binary cross entrupy\n        cost = cost_function(X, y, weights)\n        # Save the cost from each iteration\n        cost_history.append(cost)\n\n    return weights, cost_history\n\ndef predict(X, weights):\n    \"\"\"\n    Predict whether the label is 0 or 1 using learned logistic regression parameters.\n    \"\"\"\n    predictions = sigmoid(X @ weights)\n    return predictions.round()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example data\n    X = np.array([\n        [1, 2],\n        [1, 3],\n        [1, 4],\n        [1, 5]\n    ])\n    y = np.array([0, 0, 1, 1])\n\n    # Add intercept term to X\n    X = np.hstack((np.ones((X.shape[0], 1)), X))\n\n    # Initial weights (parameters)\n    initial_weights = np.zeros(X.shape[1])\n\n    # Gradient descent settings\n    iterations = 1000\n    alpha = 0.01\n\n    weights, cost_history = gradient_descent(X, y, initial_weights, alpha, iterations)\n\n    print(\"Learned weights:\", weights)\n    # Predictions\n    predictions = predict(X, weights)\n    print(\"Predictions:\", predictions)\n</code></pre>"},{"location":"engineering/","title":"Are you a Programmer?","text":"<p>Here are some learnings to enhance your coding abilities</p> <p></p> <p>CodeLLama: Potential of code generation models</p> <p>How Code generation models can enhance developer productivity?</p> <p> </p> <p></p> <p>Decorators: Decorators are like developers crown</p> <p>How to use decorators effectively?</p>"},{"location":"engineering/decorators/","title":"Decorators: Decorators are like developers crown","text":""},{"location":"engineering/decorators/#introduction-to-decorators","title":"Introduction to Decorators","text":"<p>Have you ever wanted to add extra behavior to a function without actually modifying its code? That's what decorators in Python allow you to do. Decorators act like wrappers that modify the behavior of the function they wrap. To put it simply, consider this example:</p> <pre><code>def simple_decorator(func):\n    def wrapper():\n        print(\"Something is happening before the function is called.\")\n        func()\n        print(\"Something is happening after the function is called.\")\n    return wrapper\n\n@simple_decorator\ndef say_hello():\n    print(\"Hello!\")\n\nif __name__ == '__main__':\n    say_hello()\n\n# Output:\n# Something is happening before the function is called.\n# Hello!\n# Something is happening after the function is called.\n</code></pre> <p>The @simple_decorator is altering the behavior of say_hello() by adding print statements before and after its execution.</p>"},{"location":"engineering/decorators/#what-can-decorators-do-for-you","title":"What Can Decorators Do For You?","text":"<p>Decorators serve as versatile tools, capable of logging, enforcing access control, caching, and more. They enable you to segregate responsibilities in your codebase. Essentially, they can add features to your functions or classes without altering their structure, making your code more modular and easier to manage.</p>"},{"location":"engineering/decorators/#logging-function-calls","title":"Logging Function Calls","text":"<p>Imagine you're deep into debugging, and you want to understand which functions are being called, along with the arguments they receive. Here's where a logging decorator can come in handy:</p> <pre><code>import functools\n\ndef log_function_calls(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        print(f\"Calling {func.__name__} with arguments: {args}, {kwargs}\")\n        result = func(*args, **kwargs)\n        print(f\"{func.__name__} returned: {result}\")\n        return result\n    return wrapper\n\n@log_function_calls\ndef add(a, b):\n    return a + b\n</code></pre> <p>This logs function names, arguments, and return values, saving you from littering your codebase with numerous print statements.</p>"},{"location":"engineering/decorators/#debugging-and-timing","title":"Debugging and Timing","text":"<p>You might sometimes wonder why a particular piece of code is taking too long to execute. Rather than manually calculating the execution time, why not use a decorator to do that?</p> <pre><code>import functools\nimport time\n\ndef debug_with_timing():\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            start_time = time.time()\n            result = func(*args, **kwargs)\n            end_time = time.time()\n            elapsed_time = end_time - start_time\n            print(f\"Function {func.__name__} took {elapsed_time:.6f} seconds to execute.\")\n            return result\n        return wrapper\n    return decorator\n\n@debug_with_timing\ndef long_running_function():\n    time.sleep(2)\n    return \"Done\"\n</code></pre> <p>This decorator measures the time taken by a function to execute and logs it, helping you identify bottlenecks.</p>"},{"location":"engineering/decorators/#marking-functions-as-deprecated","title":"Marking Functions as Deprecated","text":"<p>So, you've found a more efficient way of doing something and no longer want anyone to use the old method. How do you warn them? The answer is a decorator that marks the old function as deprecated.</p> <pre><code>import functools\nimport warnings\n\ndef deprecated(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        warnings.warn(f\"Function {func.__name__} is deprecated.\", \n                      category=DeprecationWarning, \n                      stacklevel=2)\n        return func(*args, **kwargs)\n    return wrapper\n\n@deprecated\ndef old_function():\n    return \"This function is outdated.\"\n</code></pre> <p>Using this decorator, a deprecation warning is issued whenever the function is called.</p>"},{"location":"engineering/decorators/#implementing-retries","title":"Implementing Retries","text":"<p>We all know that certain operations, especially the ones involving networks or databases, can fail sporadically. How about a decorator that allows your function to retry a few times before giving up?</p> <pre><code>import functools\nimport time\n\ndef retry_on_failure(max_retries=3, delay=1):\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            for _ in range(max_retries):\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    print(f\"Failed with error: {e}. Retrying in {delay} seconds...\")\n                    time.sleep(delay)\n            raise Exception(f\"Function {func.__name__} failed after {max_retries} retries.\")\n        return wrapper\n    return decorator\n\n@retry_on_failure()\ndef api_request(url):\n    # Code for making an API request goes here\n    ...\n</code></pre> <p>This decorator will attempt to re-run the function up to a specified number of times if it encounters an error, making your system more resilient to transient failures.</p>"},{"location":"engineering/decorators/#memoization","title":"Memoization","text":"<p>Have a function that gets called multiple times with the same arguments? You might be wasting computation cycles recalculating the same values again and again. Here's where memoization comes in handy:</p> <pre><code>import functools\n\ndef memoize(func):\n    cache = {}\n    @functools.wraps(func)\n    def wrapper(*args):\n        if args not in cache:\n            cache[args] = func(*args)\n        return cache[args]\n    return wrapper\n\n@memoize\ndef fibonacci(n):\n    if n &lt;= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n</code></pre> <p>This decorator caches the return value of functions, so if you call it again with the same arguments, the cached value is returned instead of re-running the function.</p>"},{"location":"engineering/decorators/#enforcing-access-control","title":"Enforcing Access Control","text":"<p>Security is a big deal. We often need to restrict who can do what in our applications. This decorator helps enforce such access controls:</p> <pre><code>import functools\n\ndef is_authenticated():\n    # Authentication check\n    ...\n\ndef requires_authentication(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        if is_authenticated():\n            return func(*args, **kwargs)\n        else:\n            raise PermissionError(\"Authentication required.\")\n    return wrapper\n\n@requires_authentication\ndef sensitive_operation():\n    # Code for sensitive operation goes here\n    ...\n</code></pre> <p>This decorator checks for authentication and only proceeds with function execution if the user is authenticated, safeguarding sensitive operations in your system.</p>"},{"location":"engineering/decorators/#conclusion","title":"Conclusion","text":"<p>Decorators offer a powerful, flexible way to modify the behavior of your functions or methods. They can be your best friend for debugging, performance tuning, access control, and many other tasks. By using them effectively, you're well on your way to writing cleaner, more efficient, and more maintainable Python code.</p> <p>Hope you found this enlightening! Happy Coding!</p>"},{"location":"engineering/potential_of_code_generation_language_models/","title":"Empowering Developers: Unleashing the Potential of Code Generation Language Models","text":"<p>We are all familiar with the impressive capabilities of ChatGPT in tasks like generating code from English descriptions or completing partial code snippets. However, a significant limitation is the dependency on online OpenAI APIs, preventing offline usage. Despite OpenAI's assurance that user data isn't used for training subsequent versions, concerns over security have restrained major companies from fully embracing these models for coding tasks.</p> <p>Enter Codellama - a dedicated AI model fine-tuned specifically for code generation. Not only is it free to use, but it's also a lightweight solution that can operate offline on local machines. This breakthrough empowers corporations to harness the prowess of language models while keeping operations secure within their networks.</p> <p>What's more, the rapid evolution in this field is undeniable. Almost immediately after Codellama's release, a finely tuned variant named WizardCoder emerged, outperforming GPT-4 on select code datasets. These advancements underline the urgency of tapping into the full spectrum of these models' capabilities to enhance overall productivity in software development.</p>"},{"location":"engineering/potential_of_code_generation_language_models/#in-practice","title":"In Practice","text":"<p>We all acknowledge that language models are renowned for their effectiveness in code completion. Yet, it's crucial to acknowledge that their potential reaches much further. These models present a diverse range of developer-centric applications that amplify productivity and streamline efficiency throughout every phase of the coding journey.</p> <p>Here are some effective ways we can leverage these models to simplify our coding tasks and achieve even greater productivity:</p>"},{"location":"engineering/potential_of_code_generation_language_models/#automated-testing","title":"Automated Testing","text":"<p>Writing unit tests and integration tests are crucial for maintaining code quality. Code generation models can assist in generating test cases and assertions based on code functionality.</p>"},{"location":"engineering/potential_of_code_generation_language_models/#code-translation","title":"Code Translation","text":"<p>For international teams, or when working with codebases in different languages, code translation can be a challenge. Code generation models can help translate code snippets from one programming language to another, saving time and effort.</p>"},{"location":"engineering/potential_of_code_generation_language_models/#refactoring-assistance","title":"Refactoring Assistance","text":"<p>Refactoring code to improve readability, performance, or maintainability can be a complex task. Code generation models can suggest refactoring options, provide alternative implementations, and highlight potential areas for improvement.</p>"},{"location":"engineering/potential_of_code_generation_language_models/#code-documentation","title":"Code Documentation","text":"<p>Writing comprehensive and clear code comments and documentation is essential for collaborative development. Code generation models can help generate explanatory comments in different languages for functions, classes, and modules, improving the overall quality of the codebase.</p>"},{"location":"engineering/potential_of_code_generation_language_models/#uml-diagram-generation","title":"UML Diagram Generation","text":"<p>Code generation models can be employed to automatically generate Unified Modeling Language (UML) diagrams from code or textual descriptions. This can include class diagrams, sequence diagrams, activity diagrams, and more, aiding in visualizing the structure and behavior of software systems. This is particularly useful for documenting complex architectures and facilitating communication between developers and stakeholders.</p>"},{"location":"engineering/potential_of_code_generation_language_models/#bug-fixes","title":"Bug Fixes","text":"<p>Developers can use code generation models to identify and fix bugs in their code. By describing the issue, the model can provide insights into potential solutions, helping to troubleshoot problems more efficiently.</p>"},{"location":"engineering/potential_of_code_generation_language_models/#code-generation","title":"Code Generation","text":"<p>Developers might sometimes have high-level ideas but struggle to translate them into code. Code generation models can take natural language descriptions of a task and generate corresponding code snippets.</p>"},{"location":"engineering/potential_of_code_generation_language_models/#code-complexity-reduction","title":"Code Complexity Reduction","text":"<p>Code generation models can provide suggestions for simplifying complex code blocks, and improving code readability and maintainability.</p>"},{"location":"engineering/potential_of_code_generation_language_models/#mock-data-generation","title":"Mock Data Generation","text":"<p>During development and testing, creating mock data can be time-consuming. Code generation models can help generate realistic mock data for testing purposes.</p>"},{"location":"engineering/potential_of_code_generation_language_models/#version-migration","title":"Version Migration","text":"<p>When migrating to a new version of a programming language or framework, code generation models can help identify and update code that needs modification due to changes in syntax or API.</p>"},{"location":"engineering/potential_of_code_generation_language_models/#api-usage-examples","title":"API Usage Examples","text":"<p>When working with new libraries or APIs, developers might struggle with understanding the correct usage. Code generation models can provide real-world examples of how to interact with different APIs, making it easier to integrate them into projects.</p>"},{"location":"engineering/potential_of_code_generation_language_models/#data-parsing-and-serialization","title":"Data Parsing and Serialization","text":"<p>Code generation models can assist in generating code to parse and serialize data in various formats such as JSON, XML, CSV, and more.</p>"},{"location":"engineering/potential_of_code_generation_language_models/#conclusion","title":"Conclusion","text":"<p>These use cases underscore the remarkable versatility of code generation language models and their potential to seamlessly integrate into diverse stages of the software development lifecycle. By leveraging these capabilities, developers can significantly enhance their efficiency, code quality, and collaborative efforts.</p> <p>However, it's important to acknowledge that code generation models cannot be solely relied upon without consideration. Instances of generating buggy code can still arise. Consequently, IDEs must undergo updates to seamlessly integrate the extensive capabilities of language models, as highlighted in the mentioned use cases. Equally vital is to ensure that developers maintain convenient access to validate the content of the generated code. Striking this balance between automation and human oversight will be pivotal in fully capitalizing on these models' potential and driving the software development landscape towards enhanced efficiency and reliability.</p> <p>Happy coding! \u263a</p>"},{"location":"generated/","title":"Generated Blogs","text":"<p>Here are some interesting blogs generated completely by OpenAI bot: Blog Helper</p> <p></p> <p>Urban Gardening Innovations</p> <p>Cultivating Green Spaces in the Concrete Jungle (Nov 2023)</p> <p> </p> <p></p> <p>Coding Newbie</p> <p>A Day in the Life of a Coding Newbie (Nov 2023)</p> <p> </p>"},{"location":"generated/coding_newbie/","title":"A Day in the Life of a Coding Newbie","text":""},{"location":"generated/coding_newbie/#the-dawn-of-discovery","title":"The Dawn of Discovery","text":"<p>Waking up with a sense of purpose, our coding newbie, Alex, starts their day bright and early. Their journey into the world of coding is fresh, filled with curiosity and a dash of nervous excitement. Today, they dive deeper into the fascinating world of programming - a skill that promises to unlock new horizons.</p> <p></p>"},{"location":"generated/coding_newbie/#morning-setting-up-for-success","title":"Morning: Setting Up for Success","text":"<p>8:00 AM - A Healthy Start:  Before delving into code, Alex knows the importance of a nutritious breakfast. A balanced meal fuels the mind for the challenges ahead.</p> <p>8:30 AM - Workspace Prep: Alex sets up their workspace: a quiet corner with a comfortable chair, their laptop ready, and headphones at hand for focus.</p> <p>9:00 AM - The Learning Begins: Opening their laptop, Alex starts with a coding tutorial on Python - a language known for its simplicity and versatility. They're using an online platform that offers interactive exercises.</p>"},{"location":"generated/coding_newbie/#midday-navigating-challenges","title":"Midday: Navigating Challenges","text":"<p>12:00 PM - Lunch Break and Reflection: After a productive morning session, it's time for a break. Alex uses this time to reflect on what they've learned and jot down questions.</p> <p>1:00 PM - Practice Makes Perfect: Post-lunch, Alex tackles a small project. Today's challenge is to create a basic calculator. They begin by sketching out the logic on paper.</p> <p>2:30 PM - Debugging: The Coder's Rite of Passage: Encountering their first major bug, Alex feels a mix of frustration and excitement. Debugging is tough but rewarding. It's a puzzle waiting to be solved.</p>"},{"location":"generated/coding_newbie/#afternoon-expanding-horizons","title":"Afternoon: Expanding Horizons","text":"<p>4:00 PM - Community Time: Alex joins an online coding community. They post questions, share their calculator project, and receive constructive feedback.</p> <p>5:00 PM - Exploring New Concepts: With the project wrapped up, Alex spends some time learning about data structures. Understanding these is key to becoming a proficient programmer.</p>"},{"location":"generated/coding_newbie/#evening-reflection-and-relaxation","title":"Evening: Reflection and Relaxation","text":"<p>7:00 PM - Dinner and Downtime: After a day of intense learning, it's time to relax. Dinner is followed by some leisure time, away from the screen.</p> <p>8:30 PM - A Quick Review: Before bed, Alex does a quick review of the day's learning. This reinforces their understanding and prepares them for the next day.</p>"},{"location":"generated/coding_newbie/#signing-off","title":"Signing Off","text":"<p>10:00 PM - Lights Out: It's been a fulfilling day. As Alex drifts off to sleep, they dream of the endless possibilities coding will unlock.</p> <p>Coding is a journey of constant learning and discovery. Each day brings new challenges, but also immense satisfaction. For Alex, and for many others, the world of coding is not just about writing lines of code; it's about thinking critically, solving problems, and being part of a community that is shaping the future.</p> <p>Remember, every expert was once a beginner. The key is to start, persevere, and keep exploring.</p>"},{"location":"generated/urban_gardening_innovations/","title":"Urban Gardening Innovations: Cultivating Green Spaces in the Concrete Jungle","text":""},{"location":"generated/urban_gardening_innovations/#introduction","title":"Introduction","text":"<p>Hello, green thumbs and city dwellers! Urban gardening has evolved from a simple hobby into a revolutionary movement. As concrete jungles expand, the need for green spaces becomes ever more critical. This blog post dives into the latest innovations in urban gardening, exploring how they're not just beautifying our cities but also contributing to sustainability and community well-being.</p>"},{"location":"generated/urban_gardening_innovations/#innovations-in-urban-gardening","title":"Innovations in Urban Gardening","text":""},{"location":"generated/urban_gardening_innovations/#vertical-gardens-sky-high-greenery","title":"Vertical Gardens: Sky-high Greenery","text":"<p>One of the most visually striking innovations in urban gardening is the vertical garden. These living walls bring a breath of fresh air to urban landscapes. By growing plants on vertical surfaces, they maximize limited space in city environments.</p> <p></p>"},{"location":"generated/urban_gardening_innovations/#hydroponic-systems-soilless-success","title":"Hydroponic Systems: Soilless Success","text":"<p>Hydroponic systems, which grow plants in a water-based, nutrient-rich solution, are a game-changer. They use less water than traditional soil gardening, and because they can be set up indoors, they allow year-round cultivation.</p> <p></p>"},{"location":"generated/urban_gardening_innovations/#community-gardens-unity-in-green","title":"Community Gardens: Unity in Green","text":"<p>Community gardens are sprouting up in cities worldwide, turning vacant lots into vibrant green spaces. These gardens do more than grow food; they foster community spirit, provide education, and create a sense of belonging.</p> <p></p>"},{"location":"generated/urban_gardening_innovations/#smart-gardening-tech-meets-terra","title":"Smart Gardening: Tech Meets Terra","text":"<p>The integration of technology in urban gardening is fascinating. Smart gardening systems can automate watering and lighting, and apps can provide reminders and tips for plant care, making gardening more accessible to everyone.</p> <p></p>"},{"location":"generated/urban_gardening_innovations/#rooftop-gardens-oasis-above","title":"Rooftop Gardens: Oasis Above","text":"<p>Rooftop gardens are turning barren urban rooftops into lush, green oases. They not only provide a space for relaxation and agriculture but also help in insulation and reducing urban heat islands.</p> <p></p>"},{"location":"generated/urban_gardening_innovations/#conclusion","title":"Conclusion","text":"<p>The innovations in urban gardening are more than just a trend; they're a testament to human ingenuity and our enduring connection to nature. These green initiatives not only add aesthetic value to our urban landscapes but also contribute to ecological sustainability and social well-being. As we continue to cultivate these tiny patches of nature amidst the concrete, we're reminded that even in the busiest cities, there's always room to grow.</p> <p>In every concrete jungle, there's a hidden garden waiting to bloom.</p>"},{"location":"notes/","title":"Learning Notes","text":""},{"location":"notes/#self-extend-llm-context-window","title":"Self-Extend LLM Context Window","text":"<p>The \"Self-Extend\" method is designed to tackle the positional out-of-distribution (OOD) issue in LLMs (with relative position encodings such as RoPE). This issue arises when LLMs encounter text sequences during inference that exceed the length of their pretraining context window. In such cases, LLMs are exposed to new relative distances not present during their pretraining phase, leading to unpredictable behaviors. (Thats because, with unseen relative positions, the attention distributions are very different compared to those within the pretraining context window length)</p> <p>The core idea of \"Self-Extend\" is to remap these unseen large relative positions to those encountered during pretraining. This is achieved using a simple floor division operation (//) as the mapping function. </p> <p>The \"Self-Extend\" method innovatively employs a dual attention mechanism to extend the context window of LLMs. This mechanism consists of two types of attention: grouped attention and normal attention.</p> <ol> <li>The grouped attention is designed to handle tokens that are at a long distance from each other. In this mechanism, the FLOOR operation is applied to the positions of these distant tokens.</li> <li>The normal attention mechanism remains unchanged from the pretraining stage and is used for tokens that are in close proximity, essentially neighbors. This aspect of the attention mechanism ensures that the model retains its original, finely-tuned ability to process and understand the immediate context around a given token.</li> </ol> <p></p> <p>Note that Self-Extend only modifies the attention mechanism during inference and it does not require any fine-tuning or training.</p> <p>More about it in the paper: LLM Maybe LongLM</p>"},{"location":"notes/#continuous-batching","title":"Continuous Batching","text":"<p>In the unoptimized method, the language model processes one sequence at a time, adding a single token (word or punctuation) in each step. For example, given the input \"Mark is quick. He moves\", the model adds one word at a time until the sequence ends. This method is slow, especially for longer texts.</p> <p>The batched generation method improves efficiency by processing multiple sequences simultaneously. Sequences are padded with filler tokens to make them equal in length, and these tokens are masked so they don't influence the generation. For instance, the model can simultaneously process multiple sentences in one pass, adding a word to each sequence.</p> <p>Continuous batching further optimizes this process. When a sequence completes, new sequences are inserted into the batch to replace the completed ones. This approach prevents wasting computational resources on generating unnecessary tokens after a sequence ends. It's more efficient because it leverages the model's capacity to handle multiple sequences in parallel, thus reducing overall processing time and resource usage.</p> <p></p> <p>More about it in the blog: Continuous batching in LLM inference</p>"},{"location":"notes/#moes-mixture-of-experts","title":"MoE's: Mixture of Experts","text":"<p>MoE models use sparse layers of specialized sub-networks (experts) and a gate network (router) that determines which tokens are sent to which expert. Each expert is specialized in different aspects of the data. In practice, the experts are FFNs, but they can also be more complex networks or even a MoE itself. </p> <p>Although a MoE might have many parameters, only some of them are used during inference. This leads to much faster inference compared to a dense model with the same number of parameters. </p> <p></p> <p>More about it in the blog: Mixture of Experts Explained</p>"},{"location":"notes/#model-merging-a-look-into-dare","title":"Model Merging: A Look into DARE","text":"<p>DARE: Introduced a simple yet impactful concept for effortlessly creating a hybrid model, by combining weights of two models from same base model finetuned on two different tasks/data. (One on Math, One on Code)</p> <p>In DARE first identify what so called delta parameters by taking difference between actual weights and finetuned weights, then pick those with small difference value may be 0.005.</p> <p>In Four Steps:</p> <ul> <li>Randomly pick may be 90% of those Delta parameters</li> <li>Drop them by resetting actual weights to zero</li> <li>Rescale remaining ones with <code>1/(1-p)</code> (where p is drop rate).</li> <li>Finally do merging with this updated weights of two or more finetuned models using any of exisiting model merging techniques.</li> </ul> <p></p> <p>More about it in the paper: Language Models are Super Mario</p>"},{"location":"research/","title":"Are you a Data Scientist?","text":"<p>Here are some learnings to enhance your technical understandings</p> <p></p> <p>Playing with Words and AI</p> <p>How Wordle Illuminates Language Model Mechanics? (Dec 2023)</p> <p> </p> <p></p> <p>LLM Decontaminator: Removing test overalap with train set</p> <p>How to detect and eleminate data overlap from train set? (Nov 2023)</p> <p> </p> <p></p> <p>Zephyr: Smaller Language Models aligning to user Intent</p> <p>How to make small language models as useful as large language models? (Nov 2023)</p> <p> </p> <p></p> <p>Step Back Prompting: How Abstraction Elevates LLM's Capabilities</p> <p>How can we make smart language models even smarter by teaching them to think abstractly? (Oct 2023)</p> <p> </p> <p></p> <p>Transformer: A Simple understanding of Transformer</p> <p>Do you want to understand the components of transformer without any jargons?</p> <p> </p> <p></p> <p>Navigating the Landscape of Modern NLP</p> <p>Do you want to get an overview of recent progress in making LLM's better? (Sep 2023)</p>"},{"location":"research/landscape_of_modern_nlp/","title":"Navigating the Landscape of Modern NLP","text":"<p>The landscape of Natural Language Processing (NLP) has been deeply transformed by the rise of Transformer models. Their unparalleled effectiveness on a wide range of tasks has led to rapid advancements in the field. However, with great power comes great responsibility, or in this case, complexity and limitations. As an experienced NLP scientist, let's delve deep into the nuances of Transformers and their recent advancements, explaining how they influence not just text generation platforms like ChatGPT but also industry-wide applications.</p>"},{"location":"research/landscape_of_modern_nlp/#vanilla-transformers","title":"Vanilla Transformers","text":"<p>The Transformer architecture has been the cornerstone of modern Natural Language Processing (NLP). It comprises several building blocks that make it uniquely suited for a variety of tasks and lets focus on few:</p> <ul> <li> <p>Positional Embeddings: These provide a way to add the sequence order information since the transformer itself doesn't understand the order of words.</p> </li> <li> <p>Attention Mechanism: This is the magic sauce. It enables the model to focus on different parts of the input text when producing an output, which is crucial for understanding the context.</p> </li> <li> <p>Activation and Normalization: Activation functions introduce non-linearity into the system, and normalization helps in faster and more stable training.</p> </li> </ul> <p>Each of these blocks plays a pivotal role in making Transformers what they are: incredibly effective but computationally demanding.</p>"},{"location":"research/landscape_of_modern_nlp/#the-long-sequence-challenges","title":"The Long Sequence Challenges","text":"<p>However, these building blocks are not without their drawbacks. The positional embeddings have limitations when it comes to very long sequences because of their fixed-size nature, and the attention mechanism is computationally intensive, having quadratic complexity. These challenges have been quite restrictive, especially when working with long texts or requiring fast real-time responses.</p>"},{"location":"research/landscape_of_modern_nlp/#tackling-attention-on-long-sequences","title":"Tackling Attention on Long Sequences:","text":"<p>These methods primarily focus on enabling the transformer architecture to manage long sequences without overwhelming computational and memory resources.</p> <ul> <li> <p>Rotary positional embeddings (RoPE): The key idea behind RoPE is to use a rotation matrix to encode absolute positions while also incorporating explicit relative position dependencies in the self-attention mechanism.</p> </li> <li> <p>Alibi positional embeddings: ALiBi proposes a much simpler relative position encoding scheme. The relative distance that input tokens have to each other is added as a negative integer scaled by a pre-defined value m to each query-key entry.</p> </li> <li> <p>Dilated Attention: This method spaces out the attention heads, allowing the model to capture long-range dependencies without having to attend to every single token. This reduces computational overhead and allows the model to work with longer sequences.</p> </li> <li> <p>Sliding Window Attention: In this technique, each token in the sequence pays attention only to a subset of nearby tokens rather than all tokens. This localized approach makes it feasible to work with longer sequences.</p> </li> <li> <p>Attention Sinks: Preserve a few initial tokens' KV alongside the sliding window's KV in the attention computation. These initial tokens serve as attention sinks (tokens that receive these unnecessary attention values), stabilizing the attention distribution and maintaining the model's performance even when the cache size is exceeded.</p> </li> </ul> <p>While advancements have addressed sequence length, Large Language Models (LLMs) still pose significant challenges during inference.</p>"},{"location":"research/landscape_of_modern_nlp/#techniques-helping-in-memory-consumption","title":"Techniques Helping in Memory Consumption","text":"<p>These methods are designed to optimize the computational efficiency of transformers, significantly reducing the memory footprint while maintaining model performance.</p> <ul> <li> <p>Flash Attention: This approach breaks down the computation into smaller chunks. It provides outputs identical to the default self-attention layer but with memory costs that increase only linearly.</p> </li> <li> <p>Key-Value Caching: This technique involves storing the key-value pairs from previous attention computations. When attending to similar sequences, the model can simply reuse these stored pairs, significantly reducing the computational load. Multi-Query and Grouped Query Attention: In Multi-Query Attention, all attention heads share a single set of key-value projection weights, optimizing memory consumption. Grouped Query Attention mitigates the quality drop associated with this by employing a limited set of distinct key-value weights for different groups of heads, thus achieving a compromise between performance and efficiency.</p> </li> <li> <p>Assisted Generation: The concept involves using a smaller 'assistant' model to rapidly generate candidate tokens. The main model then confirms these tokens.</p> </li> <li> <p>Quantization: Reduce the number of bits required to represent weights and activations of neural networks. Operating at reduced numerical precision, namely 8-bit and 4-bit, can achieve computational advantages without a considerable decline in model performance.</p> </li> </ul>"},{"location":"research/landscape_of_modern_nlp/#lightweight-models-and-fine-tuning","title":"Lightweight Models and Fine-Tuning","text":"<p>The solution lies in creating lightweight models and fine-tuning them for specific tasks. Fine-tuning helps to adapt a large pre-trained model to more specific tasks efficiently. Techniques like LoRA, Adapters, and Prefix Tuning have shown promise in this area.</p> <ul> <li>LoRA: Fine-tunes attention patterns for specific tasks.</li> <li>Adapters: Small neural modules added for task-specific fine-tuning.</li> <li>Prefix Tuning: Adds a trainable prefix to adapt the pre-trained model for specific tasks.</li> </ul>"},{"location":"research/landscape_of_modern_nlp/#unlocking-full-capabilities-prompt-engineering","title":"Unlocking Full Capabilities: Prompt Engineering","text":"<p>Now that we have well-performing, fine-tuned, and compressed models, how do we extract their full capabilities? The answer lies in Prompt Engineering.</p> <p>Prompt engineering is the practice of designing inputs for generative AI tools that will produce optimal outputs. Some of the recent advances in this area are:</p> <ul> <li> <p>Tree of thoughts: It guides language models in solving complex problems by structuring the reasoning process into a tree of intermediate thoughts or steps. These thoughts are systematically explored, evaluated, and ranked to find the most promising solutions.</p> </li> <li> <p>Prompt Breeder: It works by creating a sample population of prompts and iteratively improving the prompts by performing mutation and allowing the best prompts to survive.</p> </li> <li> <p>AutoGen: We can have multiple AI agents working together on a common task.</p> </li> </ul>"},{"location":"research/landscape_of_modern_nlp/#future-landscape","title":"Future Landscape","text":"<p>As we move forward, these advancements will continue to democratize access to LLMs, making them an integral part of our daily lives from personal assistants to advanced analytics tools. The horizon looks promising, and we are only scratching the surface.</p> <p>Thank you, and stay tuned for more!</p>"},{"location":"research/llm_decontaminator/","title":"Unveiling the Power of LLM Decontaminator in Language Models","text":"<p>Imagine preparing a student for an exam by inadvertently providing them with all the answers beforehand. It sounds like a guaranteed success, right? However, when the real test comes, this student is unprepared to tackle questions they've never seen. This scenario mirrors a significant issue in the world of Language Learning Models (LLMs) known as 'contamination.' </p> <p>Contamination occurs when a language model, like a diligent student, is exposed to the test questions (or something very similar) during its training phase. The result? An inflated sense of the model's ability to generalize and solve new problems.</p>"},{"location":"research/llm_decontaminator/#objective-and-solution-the-llm-decontaminator","title":"Objective and Solution: The LLM Decontaminator","text":"<p>A simple but effective solution to this problem is the LLM Decontaminator<sup>1</sup>. This tool is designed to identify and remove these 'pre-learned' test samples from the training data, ensuring that our LLM, like a properly trained student, truly understands the material and can apply it to novel situations.</p>"},{"location":"research/llm_decontaminator/#how-does-it-work","title":"How Does It Work?","text":"<p>1. Building the Dataset: The first step involves creating a dataset of test samples and their closest training samples using a simpler Sentence Transformer.</p> <pre><code># Code snippet for building dataset\ndef build_database(train_cases, test_cases, output_path, top_k=5):\n    model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n    train_embs = bert_encode(model, train_cases)\n    test_embs = bert_encode(model, test_cases)\n    # top_k_similarity returns top k train indices for each test sample\n    top_k_indices = top_k_similarity(test_embs, train_embs, top_k)\n    db = []\n\n    for i, test_case in enumerate(test_cases):\n        top_k_cases = [train_cases[index] for index in top_k_indices[i]]\n        db.append({\"test\": test_case, \"train\": top_k_cases})      \n\n    return db\n</code></pre> <p>2. Detecting Contamination: Next, we use GPT-4 to determine if a test sample and a training sample are essentially the same question rephrased. This is akin to asking a student if two exam questions are asking the same thing, just in different words.</p> <pre><code># Code snippet for detecting contamination\ndef detect_contamination(openai_model, question1, question2):\n    knowledge_instruct =\"\"\"I will now give you two questions. I will enclose the two questions with curly braces \\{\\}.\n        Please help me determine if the following two questions are the same.\n        Disregard the names and minor changes in word order that appear within.\n        If they are, please answer 'True', otherwise answer 'False'. Do not respond with anything else.\n        If their question prompts are very similar and, without considering the solution process, they produce the same answer, we consider them to be the same question.\n        \"\"\"\n    prompt = \"part1: \\{\\n\" + question1 + \"\\n\\}\\npart2: \\{\\n\" + question2 + \"\\n\\}\"\n    retries = 0\n    while retries &lt; 30:\n        completion = client.chat.completions.create(\n                    model=openai_model,\n                    messages=[\n                        {\"role\": \"system\", \"content\": knowledge_instruct},\n                        {\"role\": \"user\", \"content\": prompt}\n                    ],\n                    timeout=3,\n                    temperature=0.3,\n                )\n        pred = completion.choices[0].message.content\n\n        if pred == \"True\":\n            return True\n        elif pred == \"False\":\n            return False\n        retries += 1\n    return False\n</code></pre> <p>Complete working code is available in github<sup>2</sup></p>"},{"location":"research/llm_decontaminator/#results-and-impact","title":"Results and Impact","text":"<p>The use of this LLM Decontaminator has shown remarkable results. By applying it to real world training datasets, we can detect and eliminate significant overlaps with widely used benchmarks. This means we're not just training our LLM to pass the test, but to truly understand and generalize its knowledge.</p>"},{"location":"research/llm_decontaminator/#conclusion-a-leap-forward-in-language-model-training","title":"Conclusion: A Leap Forward in Language Model Training","text":"<p>In conclusion, the LLM Decontaminator represents a significant step forward in the training of language models. By ensuring that our models are learning in a contamination-free environment, we're paving the way for more reliable, effective, and genuinely intelligent language processing tools. </p> <p>This development marks not just a technical achievement, but a commitment to the integrity and advancement of AI language understanding. It's a promise that our models will be as ready for the unpredictable nature of real-world language as a well-prepared student is for the twists and turns of a challenging exam.</p> <p>Special thanks to the original authors of the LLM Decontaminator paper and tool. For further details, I highly recommend visiting their paper and the GitHub repository linked in the references.</p> <ol> <li> <p>LLM Decontaminator: https://arxiv.org/abs/2311.04850\u00a0\u21a9</p> </li> <li> <p>Decontamination tool: https://github.com/lm-sys/llm-decontaminator\u00a0\u21a9</p> </li> </ol>"},{"location":"research/step_back_prompting/","title":"The Evolution of Large Language Models: A Focus on Step-Back Prompting","text":"<p>In the continually evolving landscape of Natural Language Processing (NLP), there is a rising trend of focusing on problem-solving through multi-step reasoning. Yet, state-of-the-art language models still struggle with the intricacies of complicated multi-step reasoning tasks. Could teaching these models to \"step back\" be the answer?</p>","tags":["Prompt Engineering","LLM","NLG"]},{"location":"research/step_back_prompting/#an-innovative-approach-step-back-prompting","title":"An Innovative Approach: Step-Back Prompting","text":"<p>Step-Back Prompting<sup>1</sup> is an exciting technique that aims to guide language models to answer complex questions more accurately. Unlike other methods that try to enhance intermediate reasoning steps, this approach asks the model to take a step back to grasp high-level concepts first.</p> <p>To illustrate, consider a high-school physics question:</p> <p>What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor of 2 and the volume is increased by a factor of 8? </p> <p>The model initially pauses and asks:</p> <p>What are the physics principles behind this question?</p> <p>And responds:</p> <p>Ideal gas law: PV = nRT, where P is the pressure, V is the volume, n is the number of moles, R is the gas constant, and T is the temperature.</p> <p>Only after grounding its reasoning in the Ideal Gas Law does it proceed to answer the original question:</p> <p>We can see that the pressure has decreased by a factor of 4.</p> <p>By focusing first on the high-level concept of Ideal Gas Law, the model avoids potential reasoning errors that could occur if it tackled the question head-on.</p>","tags":["Prompt Engineering","LLM","NLG"]},{"location":"research/step_back_prompting/#whats-the-big-deal","title":"What's the Big Deal?","text":"<p>The first step is to teach LLMs to step back, and derive high-level abstractions such as concepts and first principles from the specific example. The second step is to leverage the reasoning ability to ground the solution on the high-level concepts and first principles.'</p> <p>These lines capture the essence of Step-Back Prompting. It's not just a two-step process; it's an exercise in abstraction and concrete reasoning. This technique has shown remarkable improvements on tasks that require domain-specific reasoning like Physics and Chemistry. Its efficacy has been demonstrated with performance improvements of up to 27% in various scenarios.</p> <p>We conduct a variety of analysis and find that STEP-BACK PROMPTING has strong performance improvements (up to 36%) over chain of thought (CoT) prompting and take a deep breathe (TDB) prompting.'</p> <p>The evidence is compelling; this method significantly outperforms other prompting techniques. It also corrects a large portion of the base model's errors, although some limitations in the model's reasoning capabilities still exist.</p>","tags":["Prompt Engineering","LLM","NLG"]},{"location":"research/step_back_prompting/#how-does-this-change-the-game","title":"How Does This Change the Game?","text":"<p>This approach does not just improve the performance metrics; it also provides a new angle for approaching complex tasks. It serves as a template for the future, setting the stage for more sophisticated reasoning abilities in large language models.</p> <p>In summary, Step-Back Prompting(1) seems to offer an effective avenue for improving the problem-solving capabilities of language models. By emphasizing the importance of grounding reasoning in high-level concepts and principles, this technique could very well be a stepping stone to the next big breakthrough in NLP.</p> <ol> <li>Step-back prompting involves first asking a higher-level, abstract question related to the original query to clarify underlying principles, and then using that abstract information to accurately answer the original, more complex question</li> </ol> <ol> <li> <p>Step back prompting: https://arxiv.org/abs/2310.06117\u00a0\u21a9</p> </li> </ol>","tags":["Prompt Engineering","LLM","NLG"]},{"location":"research/transformer_review/","title":"A Simple understanding of Transformer","text":"<p>The \ud835\udde7\ud835\uddff\ud835\uddee\ud835\uddfb\ud835\ude00\ud835\uddf3\ud835\uddfc\ud835\uddff\ud835\uddfa\ud835\uddf2\ud835\uddff architecture is the fundamental building block that enables LLM's to understand and generate text. Let's break it down as if we're reading a magical storybook. Each component of a Transformer model is like a unique character in this captivating tale!</p> <p>\ud835\udddc\ud835\uddfb\ud835\uddfd\ud835\ude02\ud835\ude01 \ud835\uddee\ud835\uddfb\ud835\uddf1 \ud835\udde2\ud835\ude02\ud835\ude01\ud835\uddfd\ud835\ude02\ud835\ude01: Imagine you have a magic storytelling book. You tell it a sentence (input), and it gives you a new sentence (output) that continues the story.</p> <p>\ud835\uddd8\ud835\uddfa\ud835\uddef\ud835\uddf2\ud835\uddf1\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\udddf\ud835\uddee\ud835\ude06\ud835\uddf2\ud835\uddff: First, each word in the sentence you tell the magic book gets turned into a special sticker. These stickers help the book understand what each word really means.</p> <p>\ud835\udde3\ud835\uddfc\ud835\ude00\ud835\uddf6\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb\ud835\uddee\ud835\uddf9 \ud835\uddd8\ud835\uddfb\ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4: To make sure the book knows the order of the words in your sentence, each sticker gets a number tag. So the book knows \"the cat ate\" is different from \"ate the cat.\"</p> <p>\ud835\uddd4\ud835\ude01\ud835\ude01\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\udde0\ud835\uddf2\ud835\uddf0\ud835\uddf5\ud835\uddee\ud835\uddfb\ud835\uddf6\ud835\ude00\ud835\uddfa: The book has a magical magnifying glass. It uses it to look at all the stickers (words) you gave it, but it pays more 'attention' to the most important ones for understanding the story.</p> <p>\ud835\udde0\ud835\ude02\ud835\uddf9\ud835\ude01\ud835\uddf6-\ud835\udddb\ud835\uddf2\ud835\uddee\ud835\uddf1 \ud835\uddd4\ud835\ude01\ud835\ude01\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb: Imagine not just one, but several magical magnifying glasses looking at different parts of your sentence at the same time. This helps the book get a really good understanding of what you said.</p> <p>\ud835\uddd9\ud835\uddf2\ud835\uddf2\ud835\uddf1-\ud835\uddd9\ud835\uddfc\ud835\uddff\ud835\ude04\ud835\uddee\ud835\uddff\ud835\uddf1 \ud835\udddf\ud835\uddee\ud835\ude06\ud835\uddf2\ud835\uddff: It's like the book's \"editor,\" taking all the important words from your sentence and crafting them into a polished new line for the story.</p> <p>\ud835\udde1\ud835\uddfc\ud835\uddff\ud835\uddfa\ud835\uddee\ud835\uddf9\ud835\uddf6\ud835\ude07\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb: Sometimes the book gets too excited and starts talking really loudly or softly. Normalization is like a volume knob that makes sure everything sounds just right.</p> <p>\ud835\uddd7\ud835\uddf2\ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddff: Now, the book uses another magical tool to turn its thoughts into a new sentence that it can tell you, continuing the story.</p>"},{"location":"research/wordle/","title":"Playing with Words and AI: How Wordle Illuminates Language Model Mechanics","text":"<p>Hey there! Ever wondered how tools like ChatGPT seem to understand and respond to us so well? It's like magic, right? Well, behind this magic lies a nifty trick called 'transformers' \u2013 no, not the robots, but something equally cool. These transformers are the brains behind popular language models, sifting through tons of text to learn how to chat with us. But let's be honest, diving into how they work can get pretty heavy.</p> <p>So, I had this fun idea: why not use the Wordle game to peel back the layers of these language models? If you've played Wordle, you know it's all about making smart guesses based on clues. And guess what? That's kind of like how these AI brains learn from the endless stream of data. Intrigued? Let's take this quirky journey together and uncover the secrets of AI, one Wordle guess at a time!</p> <p>Link for the Code/Notebook</p>"},{"location":"research/wordle/#explaining-wordle-the-game-of-words-and-wits","title":"Explaining Wordle: The Game of Words and Wits","text":"<p>Imagine a game that's like a mini-puzzle for your brain, where words are the pieces. That's Wordle for you! In this simple yet captivating game, you have six chances to guess a five-letter word. Each guess gives you clues: letters that are right and in the correct spot turn green, letters that are in the word but in the wrong spot turn yellow, and letters not in the word stay unhighlighted. It's a daily brain teaser that's won hearts globally, not just because it's fun, but because it challenges you to think differently with each guess. Each round is a new adventure in word-guessing, where you learn from your previous guesses and inch closer to the solution. Simple, yet intriguing, right?</p>"},{"location":"research/wordle/#unpacking-my-wordle-solver-a-mini-ai-at-play","title":"Unpacking My Wordle Solver: A Mini AI at Play","text":"<p>Now, let me introduce you to my little creation: a Wordle solver that's as clever as it is simple. The journey of this solver mirrors the way a language model sifts through information. Here's how it works:</p> <ol> <li>Starting Big: It begins by gathering all English words from a database, specifically targeting those with 5 letters \u2013 the perfect candidates for our Wordle game.</li> <li>The First Guess: A random word is picked from this list. Think of this as the initial 'hypothesis' in an experiment.</li> <li>Learning from Feedback: Each guess is evaluated based on the game's feedback. The solver then refines its word list, filtering out the words that don't match the clues.</li> <li>Scoring System: The real magic happens here. Each word is scored based on the likelihood of a letter appearing in a specific position. This scoring is akin to how a language model evaluates the probability of a word or a letter in a sentence.</li> <li>Narrowing Down: From this scored list, the top contenders are picked for the next guess. It's a process of constant learning and adapting, much like how AI models train and improve.</li> <li>Repeat and Refine: The cycle continues until the right word is found or the attempts run out.</li> </ol> Here's the python code to filter and score words <pre><code>def filter_words(words_list, guess, feedback):\n    filtered_words = words_list\n\n    letter_counts = {} # only valid letters\n    for letter, color in zip(guess, feedback):\n      if color in ['yellow', 'green']:\n        if letter not in letter_counts:\n          letter_counts[letter] = 0\n        letter_counts[letter] += 1\n\n    for i, letter in enumerate(guess):\n        if feedback[i] == 'green':\n            filtered_words = [word for word in filtered_words if word[i] == letter]\n        elif feedback[i] == 'yellow':\n            filtered_words = [word for word in filtered_words if word[i] != letter and letter in word and word.count(letter) &gt;= letter_counts[letter]]\n        elif feedback[i] == 'gray' and letter not in letter_counts:\n            filtered_words = [word for word in filtered_words if letter not in word]\n\n    return filtered_words\n\ndef score_word(word, past_guesses, past_feedbacks):\n    score = 0\n    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n\n    for i, letter in enumerate(word):\n        char_index = alphabet.index(letter)\n        # Increase score based on normalized frequency of the letter in that position\n        score += normalized_position_counts[char_index][i] * 100  # Scaling factor\n\n        for guess, feedback in zip(past_guesses, past_feedbacks):\n            if feedback[i] == 'green':\n                score += 1000 if word[i] == guess[i] else -1000  # Adjusted for impact\n            elif feedback[i] == 'yellow':\n                score += 500 if letter in guess and word[i] != guess[i] else -500\n            elif feedback[i] == 'gray':\n                score -= 500 if letter in guess else 0\n\n    return score\n</code></pre> <p>While we now have a handy tool to tackle Wordle puzzles, there's a bigger picture to consider. Imagine using this tool to play numerous Wordle games, creating a treasure trove of data. This data becomes the training ground for our transformer models.However, it's crucial to remember that our goal here is not about achieving a 100% success rate in Wordle; it's about uncovering the principles that enable these AI models to learn and adapt, mirroring the complexities of human language understanding.</p>"},{"location":"research/wordle/#approach-1-training-with-complete-word-predictions","title":"Approach 1: Training with Complete Word Predictions","text":"<p>In our first approach, the aim was to teach the model how to play Wordle, much like we humans do. So, loop playing 1000 Wordle games, using a random 5-letter word from the English corpus as the target word each time. This created a unique dataset, with a series of guess matrices as inputs and the subsequent guesses with their feedback as the expected outputs.</p> <p>For instance, if the target word was guessed correctly on the 4th try, this resulted in 3 input-output (I/O) pairs for our dataset. Each input had a shape of 5x145 (each of 145 representing 5 letters of each guess ie., 5x29, where 29 accounts for 26 English characters plus 3 colors for feedback). In cases where there were fewer than 5 guesses, the remaining rows in the input matrix were filled with zeros.</p> <p>After training the model with this dataset, below are the guesses to reach the target word TORIC:</p> <p> f i e n t </p> <p> r a a a i </p> <p> t r i t r </p> <p> t o r i r </p> <p> t o r i c </p> <p>From the predictions made in the above example, it becomes evident how the model is not just guessing but actually learning and evolving. Each round of feedback it receives is like a clue, guiding it closer to the target word. This iterative learning process demonstrates that the model has grasped the fundamental rules of Wordle \u2013 making intelligent guesses based on previous feedback and refining its strategy with each step. To really understand the sophistication of the model's outputs, we can delve into the data distribution patterns that emerge from various inputs. This analysis reveals the intricate ways in which the model adapts and predicts, highlighting its ability to mimic complex decision-making processes.</p> <p>Now the real test began. Play another 1000 Wordle games, but this time, let the model make the guesses. The fascinating part? When we plotted a stacked bar chart showing the distribution of each character's presence in each of the 5 positions, the charts from the original dataset and the model's predictions were strikingly similar. This was a clear indication of how well the transformer-based language model had learned and replicated the data distribution patterns found in the original Wordle dataset.</p> <p></p> Here's the python code for the model <pre><code>class WordleTransformer(nn.Module):\n    def __init__(self, nhead=5, num_layers=3):\n        super(WordleTransformer, self).__init__()\n        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=29 * 5,\n                                                                    nhead=nhead,\n                                                                    batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=num_layers)\n        # Flatten the output of the transformer and then project it to the required size\n        self.fc = nn.Linear(29 * 5 * 5, 29 * 5)\n\n    def forward(self, x):\n        # Reshape x to fit the transformer encoder's input shape\n        x = x.view(x.size(0), -1, 29 * 5)  # Reshape to [batch_size, 5, 29*5]\n        x = self.transformer_encoder(x)\n        x = x.view(x.size(0), -1)  # Flatten the transformer's output\n        x = self.fc(x)\n        return x.view(-1, 5, 29)  # Reshape to (batch_size, 5 letters, 26 alphabet and 3 labels)\n</code></pre> <p>However, To see the whole process in action, check out the detailed Jupyter notebook, which I'll link here for those interested in diving deeper into the data and analysis.</p> <p>In our first approach, we treated each complete word as a single token, sending five such tokens as input to the transformer model. The task for the model was to map these to a set of 5 characters, each coupled with feedback, to predict the next guess. This method, while effective for our purpose, differs from how large language models are usually trained.</p> <p>Typically, these models learn to predict the next token based on the preceding ones, where a token can be an entire word or part of a word. In natural language, each word is connected to its predecessors in some way, creating a complex web of relations and meanings. By lining up the Wordle matrix and treating each character as a separate token, we see a simplified yet striking resemblance to natural language processing. Here, like in everyday speech, each character in the guess depends on the previous characters and their associated feedback. The rules are simpler and more defined, but the fundamental concept of sequential dependence remains.</p> <p>With this understanding, let's move to the next approach. Here, we'll train our model to predict each character of the next guess individually, mirroring more closely how large language models process and generate text.</p>"},{"location":"research/wordle/#approach-2-training-to-predict-character-by-character","title":"Approach 2: Training to predict character by character","text":"<p>In this approach, we take a slightly different path. Just like before, we begin by generating a dataset. This involves playing another 1000 Wordle games with various target words using our mini AI tool. However, this time, we only consider the final guess matrix from each game for our dataset.</p> <p>Our input-output pairs are quite intriguing in this setup:</p> <ul> <li>The input starts with the first guess and its feedback, and the expected output is the first character of the next guess.</li> <li>We continue adding characters from the next guess to the input, one by one, with each subsequent character becoming the new expected output.</li> </ul> Here's a python code block that illustrates how we create this training data <pre><code>class WordleDataset(Dataset):\n    def __init__(self, io_pairs, token_to_id):\n        self.input_seqs = []\n        self.target_seqs = []\n\n        for input_sequence, _ in io_pairs:\n            # Convert the sequence into token IDs, including the special tokens\n            tokenized_sequence = [token_to_id['&lt;start&gt;']]\n            for guess, feedback in input_sequence:\n                tokenized_sequence.extend([token_to_id[char] for char in guess])\n                tokenized_sequence.extend([token_to_id[fb] for fb in feedback])\n                tokenized_sequence.append(token_to_id['&lt;word_sep&gt;'])\n            tokenized_sequence.append(token_to_id['&lt;end&gt;'])\n\n            # Shift the tokenized sequence to create the target sequence\n            shifted_sequence = tokenized_sequence[1:] + [token_to_id['&lt;pad&gt;']]\n\n            # Ensure the input and target sequences are of equal length\n            self.input_seqs.append(tokenized_sequence)\n            self.target_seqs.append(shifted_sequence)\n\n    def __len__(self):\n        return len(self.input_seqs)\n\n    def __getitem__(self, idx):\n        input_seq = torch.tensor(self.input_seqs[idx], dtype=torch.long)\n        target_seq = torch.tensor(self.target_seqs[idx], dtype=torch.long)\n        return input_seq, target_seq\n</code></pre> <p>Let's look at an example:</p> <pre><code>Input: &lt;start&gt;, q, u, e, l, l, gray, green, gray, yellow, gray, &lt;word_sep&gt;, g\nOutput: u\n</code></pre> <p>In this method, we feed the model a sequence of 12 tokens \u2013 a mix of alphabets, colors, and special words \u2013 and ask it to predict the next character, color, or special word. Through this, the model learns to make guesses character by character, considering past guesses and their feedbacks.</p> Here's the python code for the model <pre><code>def generate_square_subsequent_mask(size):\n    mask = torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1).to(device)\n    return mask\n\nclass WordleTransformer(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, num_encoder_layers, max_seq_length):\n        super(WordleTransformer, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.positional_embedding = nn.Embedding(max_seq_length, embed_dim)\n        self.layers = nn.ModuleList([CustomAttention(embed_dim, num_heads) for _ in range(num_encoder_layers)])\n        self.final_linear = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, src):        \n        src = self.embedding(src)\n\n        seq_length = src.size(1)\n        positions = torch.arange(seq_length, device=src.device).unsqueeze(0).repeat(src.size(0), 1)\n        src = src + self.positional_embedding(positions)\n\n        attention_weights = []\n        mask = generate_square_subsequent_mask(seq_length)\n\n        for layer in self.layers:\n            src, layer_attention, context_emb  = layer(src, mask)\n            attention_weights.append(layer_attention)\n\n        output = self.final_linear(src)\n\n        return output, attention_weights\n\n# Meta Params\nvocab_size = len(token_to_id)\nembed_dim = 256 \nnum_heads = 32\nnum_encoder_layers = 3\nmax_seq_length = 60\n</code></pre> <p>A sample result post-training is quite revealing:</p> <pre><code>Input sequence: &lt;start&gt;, p, o, u, n, d, green, yellow, grey, grey, green\nPredicted next 17 tokens: p, a, r, o, d, green, green, gray, green, green, &lt;word_sep&gt;, p, a, p, o, d, green\n</code></pre> <p>Notice how the model generates colors right after five characters and then a word separator, perfectly following our training pattern. This shows that it has learned the overall data pattern. More interestingly, the model's use of 'green' after 'd' in 'parod' signals to keep the same letter in the first position for the next guess \u2013 a strategy similar to how we humans approach Wordle puzzles.</p> <p>But that's not all. Let's look at more results:</p> <pre><code>Input: round [green, yellow, grey, grey, green], Predicted: r, a, l, o, d\nInput: audio [green, yellow, grey, grey, green], Predicted: a, l, o, u, o\nInput: phone [green, yellow, grey, grey, green], Predicted: p, a, h, a, e\nInput: shift [green, yellow, grey, grey, green], Predicted: s, t, h, o, t\n</code></pre> <p>These examples highlight how the model maintains green-coded letters in the same positions and shifts yellow-coded letters to increase their chance of turning green in the next guess.</p> <p>So far, we've seen what the model predicts. Next, let's dive deeper into its internals. We'll explore what happens after each layer and how attention is applied to previous tokens, visualizing the layer outputs and attention matrices at each level. Stay tuned for an eye-opening journey into the heart of the transformer!</p>"},{"location":"research/wordle/#delving-deeper-understanding-the-transformers-internals","title":"Delving Deeper: Understanding the Transformer's Internals","text":"<p>In our second approach, we observe an intriguing aspect of the transformer model \u2013 its ability to process and predict each character and color. Let's examine an example and explore what it's doing with the other tokens:</p> <p>For the input 'boxer' with the feedback ['green','gray','gray','gray','green'], the model's output across different tokens is as follows:</p> <pre><code>s, a, u, e, r, gray, gray, gray, gray, gray, &lt;word_sep&gt;, b\ns, a, u, e, r, gray, gray, gray, gray, gray, &lt;word_sep&gt;, b, u\ns, a, u, e, r, gray, gray, gray, gray, gray, &lt;word_sep&gt;, b, u, g\ns, a, u, e, r, gray, gray, gray, gray, gray, &lt;word_sep&gt;, b, u, g, a\ns, a, u, e, r, gray, gray, gray, gray, gray, &lt;word_sep&gt;, b, u, g, a, r\ns, a, u, e, r, gray, gray, gray, gray, gray, &lt;word_sep&gt;, b, u, g, a, r, green\n</code></pre> <p>In the beginning, the outputs seem random because the model is working with limited context. However, after generating five letters, it starts producing colors \u2013 a learned behavior from our dataset. Notably, when 'green' aligns with a specific letter in the input (like 'b' in 'boxer'), the model learns to replicate that letter in the same position in its next guess.</p> <p>Now, let's dive into the model's layers to see how this transformation happens:</p> <p>Let's take a journey through each layer of our model and visualize how it processes the input sequence:</p> <p>1. Embedding Layer &amp; Positional Embedding Layer:</p> <p>Here, each character gets a unique embedding, and positional information is added. The same letters at different positions have slightly different shades, indicating their unique positions.</p> <p></p> <p>2. Transformer Layer 1:</p> <p>This layer adds Wordle-specific context to each token, as seen in the varying embedding patterns for the same characters at different positions.</p> <p></p> <p>3. Transformer Layer 2:</p> <p>A noticeable shift occurs here, with letters mapped to 'green' in previous tokens showing different patterns, highlighting the model's learning to predict the next token.</p> <p></p> <p>4. Transformer Layer 3:</p> <p>Here, the difference between letters and colors becomes even more pronounced, preparing the input tokens for the final prediction. The model sharpens its focus on crucial details. Here, as it anticipates predicting 'green' for the letter 'b', we see a distinct blue shade highlighting 'b' in the heatmap. This indicates the model's recognition of 'b's correct position, influenced by the green feedback for next guess.</p> <p></p> <p>5. Linear (Output) Layer:</p> <p>This layer shows high confidence in predicting certain tokens, like 'green' after 'b', and 'word_sep' after 'green', indicating the model's learned patterns.</p> <p></p> <p>Finally, let's look at the 32 attention heads. Each head attends to different parts of the input in unique ways, demonstrating the complexity and adaptability of the transformer model.</p> <p></p> <p>For those of you eager to explore these concepts in even more depth, I invite you to check out the detailed Jupyter notebook. It's packed with additional insights and visualizations that bring these ideas to life. Dive in and satisfy your curiosity!</p>"},{"location":"research/wordle/#future-work-expanding-horizons-in-ai-and-wordle","title":"Future Work: Expanding Horizons in AI and Wordle","text":"<p>As we look ahead, the potential for further exploration and innovation in the realm of AI, particularly in relation to our Wordle-based language model, is vast and exciting. Here are some avenues for future work that promise to deepen our understanding and enhance the capabilities of these models:</p> <ol> <li> <p>Contextual Endurance: Testing the model's ability to maintain context over extended plays is crucial. We aim to see if its performance remains consistent or degrades with longer sequences, pushing the limits of its memory and learning capacity.</p> </li> <li> <p>Intermediate Layer Outputs: Exploring whether we can predict certain token-based outputs from the model's intermediate layers will provide insights into the internal processing stages and how each contributes to the final prediction.</p> </li> <li> <p>Game Rule Adaptation: By fine-tuning the model with variations in Wordle's rules, we can observe how changes in the game's structure impact the model's learning patterns and weight adjustments. This experiment will highlight the model's adaptability and resilience to rule changes.</p> </li> <li> <p>Robustness Testing: Feeding the model challenging or even misleading sequences will test its robustness and error-handling abilities. This step is essential in understanding the model's limits and areas for improvement.</p> </li> <li> <p>Architectural Experiments: Trying out different architectural changes in the model and observing their behavior will allow us to refine its design for optimal performance and efficiency.</p> </li> <li> <p>Open-Ended Exploration: The field of AI is ever-evolving, and as new ideas and technologies emerge, we will continue to integrate them into our research, ensuring that our exploration remains at the cutting edge of innovation.</p> </li> </ol> <p>One key advantage of using Wordle-like games in our AI research is their computational efficiency. Unlike the heavy processing demands of experimenting with real text and large-scale models, Wordle offers a more manageable and less time-consuming platform. This simplicity allows for more frequent and varied experiments, enabling us to test hypotheses and observe results more rapidly. It's a perfect testbed for AI experimentation, where we can tweak, adjust, and innovate without the burden of extensive computational resources.</p> <p>By harnessing the simplicity and versatility of Wordle, we can push the boundaries of AI research in a more practical and accessible way. This approach not only saves time and resources but also provides a clear and tangible framework for understanding complex AI concepts. As we continue to explore and experiment with this model, the insights gained here could very well inform and enhance our approaches to dealing with real text and larger models in the future.</p>"},{"location":"research/wordle/#conclusion-a-journey-through-ai-with-wordle-as-our-guide","title":"Conclusion: A Journey Through AI with Wordle as Our Guide","text":"<p>As we come to the end of this enlightening journey, it's clear that the world of AI and transformers is less daunting when viewed through the familiar lens of the Wordle game. We've seen how a simple yet strategic word game can mirror the complex processes of language models. Our exploration from the basic rules of Wordle to the sophisticated layers of a transformer model has shown us the adaptable and learning nature of AI. By breaking down each step, from predicting complete words to individual characters, we've uncovered how these models process information, learn from feedback, and make intelligent predictions.</p> <p>This journey is more than just understanding AI; it's about appreciating the beauty of machine learning and its ability to mirror human-like decision-making in a structured, logical manner. So, next time you play Wordle or interact with a language model, remember the intricate dance of algorithms and data behind the scenes, making each guess and response possible. And as technology continues to evolve, so will our understanding and capabilities, opening up new frontiers for innovation and discovery in the realm of AI.</p>"},{"location":"research/zephyr/","title":"The Evolution of Small Language Models: Aligning to the Intent of the user with Zephyr","text":"<p>Technological advancements in natural language models have been remarkable, culminating in large and robust systems capable of a variety of tasks. While large proprietary models are at the forefront of this evolution, smaller open models are catching up, especially in their ability to align with human intent, a crucial aspect for user experience in conversational systems.</p>","tags":["Finetuning","SLM","NLG"]},{"location":"research/zephyr/#the-need-for-intent-alignment","title":"The Need for Intent Alignment","text":"<p>One of the key issues with these models is their lack of \"intent alignment\". In layman's terms, this means that these models don't always respond in the way humans would prefer. They may generate text that is grammatically correct but doesn't quite get to the heart of what the user is asking. There have been benchmarks like MT-Bench and AlpacaEval developed to specifically measure this characteristic. According to these, proprietary models tend to outperform their open counterparts, but the cost of human annotation to improve them is exorbitant.</p>","tags":["Finetuning","SLM","NLG"]},{"location":"research/zephyr/#the-distillation-method","title":"The Distillation Method","text":"<p>Here's where distillation comes in. Distillation is essentially a \"teaching\" method where a smaller model (student) learns from a larger, more capable model (teacher). It's like learning shorthand notes from an expert's extensive lecture.</p>","tags":["Finetuning","SLM","NLG"]},{"location":"research/zephyr/#zephyr-direct-distillation-of-lm-alignment","title":"ZEPHYR -  Direct Distillation Of LM Alignment","text":"<p>Zephyr<sup>1</sup> aims to align with user intent throw three stages as below.</p> <p></p>","tags":["Finetuning","SLM","NLG"]},{"location":"research/zephyr/#the-stages-of-progress-from-dsft-to-ddpo","title":"The Stages of Progress: From dSFT to dDPO","text":"","tags":["Finetuning","SLM","NLG"]},{"location":"research/zephyr/#1-distilled-supervised-fine-tuning-dsft","title":"1. Distilled Supervised Fine-Tuning (dSFT)","text":"<p>The process begins with the Distilled Supervised Fine-Tuning (dSFT) stage. Here, the student model is trained to respond to user prompts. Traditional methods usually use a dataset comprising high-quality instructions and responses. However, this research proposes a novel twist: why not use the teacher model to generate these instructions and responses?</p> <p>Let x\u2081, . . . , x\u2c7c be a set of seed prompts, constructed to represent a diverse set of topical domains. A dataset is constructed through iterative self-prompting where the teacher is used to both respond to an instruction and refine the instruction based on the response.</p> <p>This method, also known as \"distilled SFT,\" essentially involves having the teacher model guide the training of the student model.</p>","tags":["Finetuning","SLM","NLG"]},{"location":"research/zephyr/#2-ai-feedback-through-preferences-aif","title":"2. AI Feedback through Preferences (AIF)","text":"<p>Usually, human feedback is used to further improve the alignment of these language models. However, the research introduces a twist\u2014using AI-generated preferences for distillation.</p> <p>For distillation, we instead use AI preferences from the teacher model on generated outputs from other models.</p> <p>In this method, multiple models generate responses to a set of queries. These responses are then ranked by the teacher model, creating a new dataset. This dataset includes the best and a randomly chosen lower-scoring response, forming \"triples\" of data that guide the student model in recognizing more and less favorable outputs</p>","tags":["Finetuning","SLM","NLG"]},{"location":"research/zephyr/#3-distilled-direct-preference-optimization-ddpo","title":"3. Distilled Direct Preference Optimization (dDPO)","text":"<p>Refine the student model by optimizing throw a reward function that ranks the preferred response over lesser ones.</p> <p>The goal of the final step is to refine the \u03c0dSFT by maximizing the likelihood of ranking the preferred Yw over Yl in a preference model.</p> <p>The core idea is to implement a reward function, often denoted as r\u03b8(x,y), that is influenced by the \"teacher\" model's preferences. This function forms the basis of a preference model that aims to rank a higher-scoring response  over a lower-scoring one.</p>","tags":["Finetuning","SLM","NLG"]},{"location":"research/zephyr/#an-example","title":"An Example","text":"<p>Below is a nice example<sup>2</sup> showing Zephyr's(1) ability to keep track of complex scenarios</p> <ol> <li>Zephyr is like teaching a chatbot to talk smarter by first giving it basic lessons, then getting tips from a smarter chatbot, and finally polishing its answers to be top-notch.</li> </ol> <p>I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. I also gave 3 bananas to my brother. How many apples did I remain with?</p> <p>After buying 10 apples, you gave away 2 apples to your neighbor and 2 apples to the repairman. This left you with a total of 6 apples. Then, you went back to the market and bought another 5 apples. You ate one of them, which left you with 4 more apples. In total, you now have 10 apples from your initial purchase (6 from the first time plus 4 from the second time).</p> <p>Additionally, you gave away 3 bananas to your brother. Since we don't know how many bananas you had before giving them away, we can't include them in this calculation. However, let's assume that you didn't have any bananas before going to the market. In that case, you would now have a total of 10 apples and no bananas remaining.</p> <p>To summarize, after all your transactions, you remain with a total of 10 apples but no bananas.</p>","tags":["Finetuning","SLM","NLG"]},{"location":"research/zephyr/#conclusion","title":"Conclusion","text":"<p>With the advent of methods like dDPO and models like ZEPHYR-7B, we're stepping into an era where small open language models can be as useful and aligned with human preferences as their larger, proprietary counterparts. It marks a democratization of technology where advanced conversational agents can be developed and deployed at a fraction of the cost. The future is not just about building bigger models but about making them smarter, aligned, and perhaps, more human-like.</p> <ol> <li> <p>Step back prompting: https://arxiv.org/abs/2310.16944\u00a0\u21a9</p> </li> <li> <p>Reddit post about Zephyr with examples \u21a9</p> </li> </ol>","tags":["Finetuning","SLM","NLG"]},{"location":"tech-updates/","title":"Tech Updates","text":"<p>Watch my likes in Twitter</p>"}]}