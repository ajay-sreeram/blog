[{"url": "https://twitter.com/ai2_aristo/status/1715482682854101017?s=12", "code": "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Can LLMs simulate human behavior in complex environments? How do we even test this? We release a simulation environment called AucArena for evaluating LLMs within English auctions, a setting involving many skills related to resource and risk management. <a href=\"https://t.co/4paBDCxxJk\">https://t.co/4paBDCxxJk</a> <a href=\"https://t.co/qi2sSTCldx\">https://t.co/qi2sSTCldx</a></p>&mdash; Aristo Team at AI2 (@ai2_aristo) <a href=\"https://twitter.com/ai2_aristo/status/1715482682854101017?ref_src=twsrc%5Etfw\">October 20, 2023</a></blockquote>"}, {"url": "https://twitter.com/langchainai/status/1715393748581109888?s=12", "code": "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">\u2b50\ufe0f Multi-Vector Retriever for RAG on tables, text, and images \u2b50\ufe0f<br><br>Seamless question-answering across diverse data types (images, text, tables) is one of the holy grails of RAG.<br><br>We\u2019re releasing three new cookbooks that showcase the multi-vector retriever to tackle this challenge.\u2026 <a href=\"https://t.co/T9nkDSWp4P\">pic.twitter.com/T9nkDSWp4P</a></p>&mdash; LangChain (@LangChainAI) <a href=\"https://twitter.com/LangChainAI/status/1715393748581109888?ref_src=twsrc%5Etfw\">October 20, 2023</a></blockquote>"}, {"url": "https://twitter.com/jmin__cho/status/1715082945265053782?s=12", "code": "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">\ud83c\udfa8DiagrammerGPT: a novel 2-stage text-to-diagram generation framework leveraging LLMs (e.g., GPT-4) to create fine-grained layouts for open-domain, open-platform diagrams (w/ dense objects+complex relations)!<a href=\"https://t.co/oJk7yYEUOU\">https://t.co/oJk7yYEUOU</a><a href=\"https://twitter.com/AbhayZala7?ref_src=twsrc%5Etfw\">@AbhayZala7</a> <a href=\"https://twitter.com/hanlin_hl?ref_src=twsrc%5Etfw\">@hanlin_hl</a> <a href=\"https://twitter.com/mohitban47?ref_src=twsrc%5Etfw\">@mohitban47</a> <a href=\"https://twitter.com/uncnlp?ref_src=twsrc%5Etfw\">@uncnlp</a><br>\ud83e\uddf5 <a href=\"https://t.co/1lSJBGgm9k\">https://t.co/1lSJBGgm9k</a> <a href=\"https://t.co/aTWy9oMaNn\">pic.twitter.com/aTWy9oMaNn</a></p>&mdash; Jaemin Cho (@jmin__cho) <a href=\"https://twitter.com/jmin__cho/status/1715082945265053782?ref_src=twsrc%5Etfw\">October 19, 2023</a></blockquote>"}, {"url": "https://twitter.com/_akhaliq/status/1715237306506813678?s=12", "code": "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">AutoMix: Automatically Mixing Language Models<br><br>paper page: <a href=\"https://t.co/Mz04TCOlH8\">https://t.co/Mz04TCOlH8</a><br><br>Large language models (LLMs) are now available in various sizes and configurations from cloud API providers. While this diversity offers a broad spectrum of choices, effectively leveraging the\u2026 <a href=\"https://t.co/geo4sXLnhE\">pic.twitter.com/geo4sXLnhE</a></p>&mdash; AK (@_akhaliq) <a href=\"https://twitter.com/_akhaliq/status/1715237306506813678?ref_src=twsrc%5Etfw\">October 20, 2023</a></blockquote>"}, {"url": "https://twitter.com/haihaoshen/status/1715335763032780853?s=12", "code": "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">\ud83d\udce2StreamingLLM landed in Intel Extension for Transformers to support LLM inference infinity on CPU, up to 4M tokens! <br>\ud83c\udfafCheck out the code: <a href=\"https://t.co/oWYnKP6OQt\">https://t.co/oWYnKP6OQt</a>, search &quot;StreamingLLM&quot; and have a try!<a href=\"https://twitter.com/hashtag/oneapi?src=hash&amp;ref_src=twsrc%5Etfw\">#oneapi</a> <a href=\"https://twitter.com/intel?ref_src=twsrc%5Etfw\">@intel</a> <a href=\"https://twitter.com/huggingface?ref_src=twsrc%5Etfw\">@huggingface</a> <a href=\"https://twitter.com/Guangxuan_Xiao?ref_src=twsrc%5Etfw\">@Guangxuan_Xiao</a> <a href=\"https://twitter.com/_akhaliq?ref_src=twsrc%5Etfw\">@_akhaliq</a></p>&mdash; Haihao Shen (@HaihaoShen) <a href=\"https://twitter.com/HaihaoShen/status/1715335763032780853?ref_src=twsrc%5Etfw\">October 20, 2023</a></blockquote>"}]