[{"url": "https://twitter.com/tsengalb99/status/1733222467953422702?s=12", "code": "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">\ud83e\uddf5 (1/n)<br><br>\ud83d\udc49 Introducing QuIP#, a new SOTA LLM quantization method that uses incoherence processing from QuIP &amp; lattices to achieve 2 bit LLMs with near-fp16 performance! Now you can run LLaMA 2 70B on a 24G GPU w/out offloading!<br><br>\ud83d\udcbb <a href=\"https://t.co/eM8Br2CqL3\">https://t.co/eM8Br2CqL3</a> <a href=\"https://t.co/BviPUOedKW\">pic.twitter.com/BviPUOedKW</a></p>&mdash; Albert Tseng @ NeurIPS (@tsengalb99) <a href=\"https://twitter.com/tsengalb99/status/1733222467953422702?ref_src=twsrc%5Etfw\">December 8, 2023</a></blockquote>"}, {"url": "https://twitter.com/theturingpost/status/1730572629860569279?s=12", "code": "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Adapters a new open-source library for transfer learning in LLMs.<br><br>Features:<br><br>\u25aa\ufe0f Unifies 10 adapter methods in one interface<br>\u25aa\ufe0f Facilitates easy use and flexible configurations<br>\u25aa\ufe0f Enables complex adapter setups through composition blocks<br><br>Links \ud83d\udc47\ud83c\udffc <a href=\"https://t.co/W2ZQ9evJ4Y\">pic.twitter.com/W2ZQ9evJ4Y</a></p>&mdash; TuringPost (@TheTuringPost) <a href=\"https://twitter.com/TheTuringPost/status/1730572629860569279?ref_src=twsrc%5Etfw\">December 1, 2023</a></blockquote>"}, {"url": "https://twitter.com/adapterhub/status/1728008202912989357?s=12", "code": "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">\ud83c\udf89 Exciting news! The new Adapters library for modular and parameter-efficient transfer learning is out!  \ud83e\udd16<br><br>Now simplified &amp; disentangled from <a href=\"https://twitter.com/huggingface?ref_src=twsrc%5Etfw\">@huggingface</a><br><br>pip install adapters<br>pip install transformers<br><br>\ud83d\udcc4<a href=\"https://t.co/YUxmvjAf72\">https://t.co/YUxmvjAf72</a><br><br>\ud83d\udc7e <a href=\"https://t.co/GTekd4MEFS\">https://t.co/GTekd4MEFS</a><a href=\"https://twitter.com/hashtag/EMNLP2023?src=hash&amp;ref_src=twsrc%5Etfw\">#EMNLP2023</a><br><br>\ud83e\uddf5\ud83d\udc47 <a href=\"https://t.co/rbHW7hTeoG\">pic.twitter.com/rbHW7hTeoG</a></p>&mdash; AdapterHub (@AdapterHub) <a href=\"https://twitter.com/AdapterHub/status/1728008202912989357?ref_src=twsrc%5Etfw\">November 24, 2023</a></blockquote>"}, {"url": "https://twitter.com/adapterhub/status/1728008202912989357?s=12", "code": "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">\ud83c\udf89 Exciting news! The new Adapters library for modular and parameter-efficient transfer learning is out!  \ud83e\udd16<br><br>Now simplified &amp; disentangled from <a href=\"https://twitter.com/huggingface?ref_src=twsrc%5Etfw\">@huggingface</a><br><br>pip install adapters<br>pip install transformers<br><br>\ud83d\udcc4<a href=\"https://t.co/YUxmvjAf72\">https://t.co/YUxmvjAf72</a><br><br>\ud83d\udc7e <a href=\"https://t.co/GTekd4MEFS\">https://t.co/GTekd4MEFS</a><a href=\"https://twitter.com/hashtag/EMNLP2023?src=hash&amp;ref_src=twsrc%5Etfw\">#EMNLP2023</a><br><br>\ud83e\uddf5\ud83d\udc47 <a href=\"https://t.co/rbHW7hTeoG\">pic.twitter.com/rbHW7hTeoG</a></p>&mdash; AdapterHub (@AdapterHub) <a href=\"https://twitter.com/AdapterHub/status/1728008202912989357?ref_src=twsrc%5Etfw\">November 24, 2023</a></blockquote>"}, {"url": "https://twitter.com/lmsysorg/status/1724861980123844925?s=12", "code": "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Announcing S-LoRA: Serving **Thousands** of LoRA Adapters with a Single A100!<br><br>S-LoRA boosts throughput by up to 4x and significantly increase concurrent adapter support by orders of magnitude, when comparing against baselines (PEFT/vLLM-packed).<br><br>Use cases? S-LoRA enables\u2026 <a href=\"https://t.co/rcAcvyMfAP\">pic.twitter.com/rcAcvyMfAP</a></p>&mdash; lmsys.org (@lmsysorg) <a href=\"https://twitter.com/lmsysorg/status/1724861980123844925?ref_src=twsrc%5Etfw\">November 15, 2023</a></blockquote>"}]