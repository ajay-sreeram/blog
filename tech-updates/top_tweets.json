[{"url": "https://twitter.com/adapterhub/status/1728008202912989357?s=12", "code": "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">\ud83c\udf89 Exciting news! The new Adapters library for modular and parameter-efficient transfer learning is out!  \ud83e\udd16<br><br>Now simplified &amp; disentangled from <a href=\"https://twitter.com/huggingface?ref_src=twsrc%5Etfw\">@huggingface</a><br><br>pip install adapters<br>pip install transformers<br><br>\ud83d\udcc4<a href=\"https://t.co/YUxmvjAf72\">https://t.co/YUxmvjAf72</a><br><br>\ud83d\udc7e <a href=\"https://t.co/GTekd4MEFS\">https://t.co/GTekd4MEFS</a><a href=\"https://twitter.com/hashtag/EMNLP2023?src=hash&amp;ref_src=twsrc%5Etfw\">#EMNLP2023</a><br><br>\ud83e\uddf5\ud83d\udc47 <a href=\"https://t.co/rbHW7hTeoG\">pic.twitter.com/rbHW7hTeoG</a></p>&mdash; AdapterHub (@AdapterHub) <a href=\"https://twitter.com/AdapterHub/status/1728008202912989357?ref_src=twsrc%5Etfw\">November 24, 2023</a></blockquote>"}, {"url": "https://twitter.com/lmsysorg/status/1724861980123844925?s=12", "code": "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Announcing S-LoRA: Serving **Thousands** of LoRA Adapters with a Single A100!<br><br>S-LoRA boosts throughput by up to 4x and significantly increase concurrent adapter support by orders of magnitude, when comparing against baselines (PEFT/vLLM-packed).<br><br>Use cases? S-LoRA enables\u2026 <a href=\"https://t.co/rcAcvyMfAP\">pic.twitter.com/rcAcvyMfAP</a></p>&mdash; lmsys.org (@lmsysorg) <a href=\"https://twitter.com/lmsysorg/status/1724861980123844925?ref_src=twsrc%5Etfw\">November 15, 2023</a></blockquote>"}, {"url": "https://twitter.com/abacaj/status/1724850925741768767?s=12", "code": "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Satya announcing Phi-2, says will be open source \ud83e\udd14, and is 50% better at mathematical reasoning <a href=\"https://t.co/y2UyrRdvCu\">pic.twitter.com/y2UyrRdvCu</a></p>&mdash; anton (@abacaj) <a href=\"https://twitter.com/abacaj/status/1724850925741768767?ref_src=twsrc%5Etfw\">November 15, 2023</a></blockquote>"}, {"url": "https://twitter.com/maxbraun/status/1724834654555935123?s=12", "code": "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Hack of the day: Llama on a microcontroller<br><br>\ud83e\udd99\ud83d\udd2c<br><br>Details and source code at <a href=\"https://t.co/CHzkyqIArh\">https://t.co/CHzkyqIArh</a><br><br>Thanks to <a href=\"https://twitter.com/karpathy?ref_src=twsrc%5Etfw\">@karpathy</a>, whose llama2.c inspired this! <a href=\"https://t.co/0B6E14tYCW\">pic.twitter.com/0B6E14tYCW</a></p>&mdash; Max Braun (@maxbraun) <a href=\"https://twitter.com/maxbraun/status/1724834654555935123?ref_src=twsrc%5Etfw\">November 15, 2023</a></blockquote>"}, {"url": "https://twitter.com/sanchitgandhi99/status/1725224506648903695?s=12", "code": "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Training code release!<br><br>Distil your own Whisper model in 3\ufe0f\u20e3 steps:<br><br>1. Pseudo-label the audio data<br>2. Shrink the teacher into a student model<br>3. Train the student on the knowledge distillation objective<br><br>Training code and examples at: <a href=\"https://t.co/ypgzwKy6oP\">https://t.co/ypgzwKy6oP</a> <a href=\"https://t.co/n0uA8W8pkS\">pic.twitter.com/n0uA8W8pkS</a></p>&mdash; Sanchit Gandhi (@sanchitgandhi99) <a href=\"https://twitter.com/sanchitgandhi99/status/1725224506648903695?ref_src=twsrc%5Etfw\">November 16, 2023</a></blockquote>"}]