
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A deep dive into the matrix of tech and code">
      
      
        <meta name="author" content="Sreeram Ajay">
      
      
      
      
      
      <link rel="icon" href="../../my_icon_nobg.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.4.4">
    
    
      
        <title>Navigating the Landscape of Modern NLP - Code & Conversations</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.bd3936ea.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.356b1318.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../styles/custom.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="blue">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#navigating-the-landscape-of-modern-nlp" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Code &amp; Conversations" class="md-header__button md-logo" aria-label="Code & Conversations" data-md-component="logo">
      
  <img src="../../my_icon_nobg.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Code & Conversations
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Navigating the Landscape of Modern NLP
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../" class="md-tabs__link">
        
  
    
  
  Research

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../coding/" class="md-tabs__link">
        
  
    
  
  Coding

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../learning/" class="md-tabs__link">
        
  
    
  
  Learning

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../tech-updates/" class="md-tabs__link">
        
  
    
  
  Tech Updates

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../generated/" class="md-tabs__link">
        
  
    
  
  Generated

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../about/" class="md-tabs__link">
        
  
    
  
  About

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Code &amp; Conversations" class="md-nav__button md-logo" aria-label="Code & Conversations" data-md-component="logo">
      
  <img src="../../my_icon_nobg.png" alt="logo">

    </a>
    Code & Conversations
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Research
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../coding/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Coding
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Learning
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../tech-updates/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tech Updates
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../generated/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Generated
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../about/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    About
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#vanilla-transformers" class="md-nav__link">
    Vanilla Transformers
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-long-sequence-challenges" class="md-nav__link">
    The Long Sequence Challenges
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tackling-attention-on-long-sequences" class="md-nav__link">
    Tackling Attention on Long Sequences:
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#techniques-helping-in-memory-consumption" class="md-nav__link">
    Techniques Helping in Memory Consumption
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lightweight-models-and-fine-tuning" class="md-nav__link">
    Lightweight Models and Fine-Tuning
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#unlocking-full-capabilities-prompt-engineering" class="md-nav__link">
    Unlocking Full Capabilities: Prompt Engineering
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#future-landscape" class="md-nav__link">
    Future Landscape
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="navigating-the-landscape-of-modern-nlp">Navigating the Landscape of Modern NLP</h1>
<p><img alt="Transformer" src="../images/landscape_nlp_1/title2.png" style="height:200px; width:200px; display: block;margin-left: auto;margin-right: auto;" /></p>
<p>The landscape of Natural Language Processing (NLP) has been deeply transformed by the rise of Transformer models. Their unparalleled effectiveness on a wide range of tasks has led to rapid advancements in the field. However, with great power comes great responsibility, or in this case, complexity and limitations. As an experienced NLP scientist, let's delve deep into the nuances of Transformers and their recent advancements, explaining how they influence not just text generation platforms like ChatGPT but also industry-wide applications.</p>
<h2 id="vanilla-transformers">Vanilla Transformers</h2>
<p>The Transformer architecture has been the cornerstone of modern Natural Language Processing (NLP). It comprises several building blocks that make it uniquely suited for a variety of tasks and lets focus on few:</p>
<ul>
<li>
<p><strong>Positional Embeddings</strong>: These provide a way to add the sequence order information since the transformer itself doesn't understand the order of words.</p>
</li>
<li>
<p><strong>Attention Mechanism</strong>: This is the magic sauce. It enables the model to focus on different parts of the input text when producing an output, which is crucial for understanding the context.</p>
</li>
<li>
<p><strong>Activation and Normalization</strong>: Activation functions introduce non-linearity into the system, and normalization helps in faster and more stable training.</p>
</li>
</ul>
<p>Each of these blocks plays a pivotal role in making Transformers what they are: incredibly effective but computationally demanding.</p>
<hr />
<h2 id="the-long-sequence-challenges">The Long Sequence Challenges</h2>
<p>However, these building blocks are not without their drawbacks. The positional embeddings have limitations when it comes to very long sequences because of their fixed-size nature, and the attention mechanism is computationally intensive, having quadratic complexity. These challenges have been quite restrictive, especially when working with long texts or requiring fast real-time responses.</p>
<hr />
<h2 id="tackling-attention-on-long-sequences">Tackling Attention on Long Sequences:</h2>
<p>These methods primarily focus on enabling the transformer architecture to manage long sequences without overwhelming computational and memory resources.</p>
<ul>
<li>
<p><strong>Rotary positional embeddings (RoPE)</strong>: The key idea behind RoPE is to use a rotation matrix to encode absolute positions while also incorporating explicit relative position dependencies in the self-attention mechanism.</p>
</li>
<li>
<p><strong>Alibi positional embeddings</strong>: ALiBi proposes a much simpler relative position encoding scheme. The relative distance that input tokens have to each other is added as a negative integer scaled by a pre-defined value m to each query-key entry.</p>
</li>
<li>
<p><strong>Dilated Attention</strong>: This method spaces out the attention heads, allowing the model to capture long-range dependencies without having to attend to every single token. This reduces computational overhead and allows the model to work with longer sequences.</p>
</li>
<li>
<p><strong>Sliding Window Attention</strong>: In this technique, each token in the sequence pays attention only to a subset of nearby tokens rather than all tokens. This localized approach makes it feasible to work with longer sequences.</p>
</li>
<li>
<p><strong>Attention Sinks</strong>: Preserve a few initial tokens' KV alongside the sliding window's KV in the attention computation. These initial tokens serve as attention sinks (tokens that receive these unnecessary attention values), stabilizing the attention distribution and maintaining the model's performance even when the cache size is exceeded.</p>
</li>
</ul>
<p>While advancements have addressed sequence length, Large Language Models (LLMs) still pose significant challenges during inference.</p>
<hr />
<h2 id="techniques-helping-in-memory-consumption">Techniques Helping in Memory Consumption</h2>
<p>These methods are designed to optimize the computational efficiency of transformers, significantly reducing the memory footprint while maintaining model performance.</p>
<ul>
<li>
<p><strong>Flash Attention</strong>: This approach breaks down the computation into smaller chunks. It provides outputs identical to the default self-attention layer but with memory costs that increase only linearly.</p>
</li>
<li>
<p><strong>Key-Value Caching</strong>: This technique involves storing the key-value pairs from previous attention computations. When attending to similar sequences, the model can simply reuse these stored pairs, significantly reducing the computational load.
Multi-Query and Grouped Query Attention: In Multi-Query Attention, all attention heads share a single set of key-value projection weights, optimizing memory consumption. Grouped Query Attention mitigates the quality drop associated with this by employing a limited set of distinct key-value weights for different groups of heads, thus achieving a compromise between performance and efficiency.</p>
</li>
<li>
<p><strong>Assisted Generation</strong>: The concept involves using a smaller 'assistant' model to rapidly generate candidate tokens. The main model then confirms these tokens.</p>
</li>
<li>
<p><strong>Quantization</strong>: Reduce the number of bits required to represent weights and activations of neural networks. Operating at reduced numerical precision, namely 8-bit and 4-bit, can achieve computational advantages without a considerable decline in model performance.</p>
</li>
</ul>
<hr />
<h2 id="lightweight-models-and-fine-tuning">Lightweight Models and Fine-Tuning</h2>
<p>The solution lies in creating lightweight models and fine-tuning them for specific tasks. Fine-tuning helps to adapt a large pre-trained model to more specific tasks efficiently. Techniques like LoRA, Adapters, and Prefix Tuning have shown promise in this area.</p>
<ul>
<li><strong>LoRA</strong>: Fine-tunes attention patterns for specific tasks.</li>
<li><strong>Adapters</strong>: Small neural modules added for task-specific fine-tuning.</li>
<li><strong>Prefix Tuning</strong>: Adds a trainable prefix to adapt the pre-trained model for specific tasks.</li>
</ul>
<hr />
<h2 id="unlocking-full-capabilities-prompt-engineering">Unlocking Full Capabilities: Prompt Engineering</h2>
<p>Now that we have well-performing, fine-tuned, and compressed models, how do we extract their full capabilities? The answer lies in Prompt Engineering.</p>
<p>Prompt engineering is the practice of designing inputs for generative AI tools that will produce optimal outputs. Some of the recent advances in this area are:</p>
<ul>
<li>
<p><strong>Tree of thoughts</strong>: It guides language models in solving complex problems by structuring the reasoning process into a tree of intermediate thoughts or steps. These thoughts are systematically explored, evaluated, and ranked to find the most promising solutions.</p>
</li>
<li>
<p><strong>Prompt Breeder</strong>: It works by creating a sample population of prompts and iteratively improving the prompts by performing mutation and allowing the best prompts to survive.</p>
</li>
<li>
<p><strong>AutoGen</strong>: We can have multiple AI agents working together on a common task.</p>
</li>
</ul>
<hr />
<h2 id="future-landscape">Future Landscape</h2>
<p>As we move forward, these advancements will continue to democratize access to LLMs, making them an integral part of our daily lives from personal assistants to advanced analytics tools. The horizon looks promising, and we are only scratching the surface.</p>
<p>Thank you, and stay tuned for more!</p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "content.code.copy", "content.code.select"], "search": "../../assets/javascripts/workers/search.f886a092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.94c44541.min.js"></script>
      
        <script src="../../scripts/twitter_widgets.js"></script>
      
        <script src="../../scripts/embed_tweets.js"></script>
      
    
  </body>
</html>